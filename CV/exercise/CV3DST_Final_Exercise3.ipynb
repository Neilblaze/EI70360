{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# MOT16 Challenge\n",
    "\n",
    "### Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Conda\\envs\\i2dl\\lib\\site-packages\\ipykernel_launcher.py:16: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "root_dir = \"./cv3dst_exercise/\"\n",
    "gnn_root_dir = \"./cv3dst_gnn_exercise/\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(gnn_root_dir, 'src'))\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tracker.data_track import MOT16Sequences\n",
    "from tracker.tracker import Tracker, ReIDTracker\n",
    "from tracker.utils import run_tracker, cosine_distance, colors\n",
    "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
    "from collections import defaultdict\n",
    "import os.path as osp\n",
    "\n",
    "import motmetrics as mm\n",
    "mm.lap.default_solver = 'lap'\n",
    "\n",
    "train_db = torch.load(osp.join(gnn_root_dir, 'data/preprocessed_data_train_2.pth')) # training set with pre-computed boxes and reid embeddings\n",
    "test_db = torch.load(osp.join(gnn_root_dir, 'data/preprocessed_data_test_2.pth'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MPNTracker Class\n",
    "overriding from the base class:\n",
    "- data_association: now uses AssignmentSimilarityNet to compute distance matrix between current frame detection and existing tracks\n",
    "- update_tracks: taking into account the tolerance, meaning that the unmatched tracks are not deleted immediately but kept for up to tol frames\n",
    "- update_results: only store active track boxes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "_UNMATCHED_COST=255\n",
    "\n",
    "class MPNTracker(ReIDTracker):\n",
    "    def __init__(self, patience, assign_net, *args, **kwargs):\n",
    "        self.assign_net = assign_net\n",
    "        self.patience = patience\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def data_association(self, boxes, scores, pred_features):\n",
    "        if self.tracks:\n",
    "            track_boxes = torch.stack([t.box for t in self.tracks], axis=0).cuda()\n",
    "            track_features = torch.stack([t.get_feature() for t in self.tracks], axis=0).cuda()\n",
    "\n",
    "            # Hacky way to recover the timestamps of boxes and tracks\n",
    "            curr_t = self.im_index * torch.ones((pred_features.shape[0],)).cuda()\n",
    "            track_t = torch.as_tensor([self.im_index - t.inactive - 1 for t in self.tracks]).cuda()\n",
    "\n",
    "            ########################\n",
    "            #### TODO starts\n",
    "            ########################\n",
    "\n",
    "            # Do a forward pass through self.assign_net to obtain our costs.\n",
    "            # Note: self.assign_net will return unnormalized probabilities.\n",
    "            # Make sure to apply the sigmoid function to them!\n",
    "\n",
    "            pred_sim = self.assign_net(track_features, pred_features.cuda(), track_boxes, boxes.cuda(), track_t, curr_t)\n",
    "            pred_sim = torch.sigmoid(pred_sim)\n",
    "\n",
    "            ########################\n",
    "            #### TODO ends\n",
    "            ########################\n",
    "\n",
    "            pred_sim = pred_sim[-1].cpu().numpy()  # Use predictions at last message passing step\n",
    "            distance = (1- pred_sim)\n",
    "\n",
    "            # Do not allow matches when sim < 0.5, to avoid low-confident associations\n",
    "            distance = np.where(pred_sim < 0.5, _UNMATCHED_COST, distance)\n",
    "\n",
    "            # Perform Hungarian matching.\n",
    "            row_idx, col_idx = linear_assignment(distance)\n",
    "            self.update_tracks(row_idx, col_idx,distance, boxes, scores, pred_features)\n",
    "\n",
    "\n",
    "        else:\n",
    "            # No tracks exist.\n",
    "            self.add(boxes, scores, pred_features)\n",
    "\n",
    "    def update_results(self):\n",
    "        \"\"\"Only store boxes for tracks that are active\"\"\"\n",
    "        for t in self.tracks:\n",
    "            if t.id not in self.results.keys():\n",
    "                self.results[t.id] = {}\n",
    "            if t.inactive == 0: # Only change\n",
    "                self.results[t.id][self.im_index] = np.concatenate([t.box.cpu().numpy(), np.array([t.score])])\n",
    "\n",
    "        self.im_index += 1\n",
    "\n",
    "    def update_tracks(self, row_idx, col_idx, distance, boxes, scores, pred_features):\n",
    "        track_ids = [t.id for t in self.tracks]\n",
    "\n",
    "        unmatched_track_ids = []\n",
    "        seen_track_ids = []\n",
    "        seen_box_idx = []\n",
    "        for track_idx, box_idx in zip(row_idx, col_idx):\n",
    "            costs = distance[track_idx, box_idx]\n",
    "            internal_track_id = track_ids[track_idx]\n",
    "            seen_track_ids.append(internal_track_id)\n",
    "            if costs == _UNMATCHED_COST:\n",
    "                unmatched_track_ids.append(internal_track_id)\n",
    "\n",
    "            else:\n",
    "                self.tracks[track_idx].box = boxes[box_idx]\n",
    "                self.tracks[track_idx].add_feature(pred_features[box_idx])\n",
    "\n",
    "                # Note: the track is matched, therefore, inactive is set to 0\n",
    "                self.tracks[track_idx].inactive=0\n",
    "                seen_box_idx.append(box_idx)\n",
    "\n",
    "\n",
    "        unseen_track_ids = set(track_ids) - set(seen_track_ids)\n",
    "        unmatched_track_ids.extend(list(unseen_track_ids))\n",
    "        ##################\n",
    "        ### TODO starts\n",
    "        ##################\n",
    "\n",
    "        # Update the `inactive` attribute for those tracks that have\n",
    "        # not been matched. kill those for which the inactive parameter\n",
    "        # is > self.patience\n",
    "        new_tracks = [];\n",
    "        for t in self.tracks:\n",
    "            if t.id in unmatched_track_ids:\n",
    "                t.inactive += 1\n",
    "            if t.inactive <= self.patience:\n",
    "                new_tracks.append(t)\n",
    "\n",
    "        self.tracks = new_tracks\n",
    "        ##################\n",
    "        ### TODO ends\n",
    "        ##################\n",
    "\n",
    "        new_boxes_idx = set(range(len(boxes))) - set(seen_box_idx)\n",
    "        new_boxes = [boxes[i] for i in new_boxes_idx]\n",
    "        new_scores = [scores[i] for i in new_boxes_idx]\n",
    "        new_features = [pred_features[i] for i in new_boxes_idx]\n",
    "        self.add(new_boxes, new_scores, new_features)\n",
    "\n",
    "    # new data_association for full MLP network\n",
    "    def data_association_new(self, boxes, scores, pred_features, isLastFrame):\n",
    "        if self.tracks:\n",
    "            track_boxes = torch.stack([t.box for t in self.tracks], axis=0).cuda()\n",
    "            track_features = torch.stack([t.get_feature() for t in self.tracks], axis=0).cuda()\n",
    "\n",
    "            # Hacky way to recover the timestamps of boxes and tracks\n",
    "            curr_t = self.im_index * torch.ones((pred_features.shape[0],)).cuda()\n",
    "            track_t = torch.as_tensor([self.im_index - t.inactive - 1 for t in self.tracks]).cuda()\n",
    "\n",
    "            ########################\n",
    "            #### TODO starts\n",
    "            ########################\n",
    "\n",
    "            # Do a forward pass through self.assign_net to obtain our costs.\n",
    "            # Note: self.assign_net will return unnormalized probabilities.\n",
    "            # Make sure to apply the sigmoid function to them!\n",
    "\n",
    "            pred_sim = self.assign_net(track_features, pred_features.cuda(), track_boxes, boxes.cuda(), track_t, curr_t)\n",
    "            pred_sim = torch.sigmoid(pred_sim)\n",
    "\n",
    "            ########################\n",
    "            #### TODO ends\n",
    "            ########################\n",
    "\n",
    "            pred_sim = pred_sim[-1].cpu().numpy()  # Use predictions at last message passing step\n",
    "            distance = (1- pred_sim)\n",
    "\n",
    "            # Do not allow matches when sim < 0.5, to avoid low-confident associations\n",
    "            distance = np.where(pred_sim < 0.5, _UNMATCHED_COST, distance)\n",
    "\n",
    "            # Perform Hungarian matching.\n",
    "            row_idx, col_idx = linear_assignment(distance)\n",
    "            self.update_tracks(row_idx, col_idx,distance, boxes, scores, pred_features)\n",
    "\n",
    "\n",
    "        else:\n",
    "            # No tracks exist.\n",
    "            self.add(boxes, scores, pred_features)\n",
    "\n",
    "    # new step for full MLP network\n",
    "    def step_new(self, frame, isLastFrame=False):\n",
    "        \"\"\"This function should be called every timestep to perform tracking with a blob\n",
    "        containing the image information.\n",
    "        \"\"\"\n",
    "        boxes = frame['det']['boxes']\n",
    "        scores = frame['det']['scores']\n",
    "        reid_feats= frame['det']['reid'].cpu()\n",
    "        self.data_association_new(boxes, scores, reid_feats, isLastFrame)\n",
    "        # new update_results()\n",
    "        for t in self.tracks:\n",
    "            if t.id not in self.results.keys():\n",
    "                self.results[t.id] = {}\n",
    "            if t.inactive == 0: # Only change\n",
    "                self.results[t.id][self.im_index] = np.concatenate([t.box.cpu().numpy(), np.array([t.score])])\n",
    "\n",
    "            self.im_index += 1\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## AssignNet\n",
    "### BipartiteNeuralMessagePassingLayer\n",
    "network for one message passing step"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class BipartiteNeuralMessagePassingLayer(nn.Module):\n",
    "    def __init__(self, node_dim, edge_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "\n",
    "        edge_in_dim  = 2*node_dim + 2*edge_dim # 2*edge_dim since we always concatenate initial edge features\n",
    "        self.edge_mlp = nn.Sequential(*[nn.Linear(edge_in_dim, int(edge_in_dim/2)), nn.ReLU(), nn.Dropout(dropout),\n",
    "                                        nn.Linear(int(edge_in_dim/2), edge_dim), nn.ReLU(), nn.Dropout(dropout)])\n",
    "\n",
    "        node_in_dim  = node_dim + edge_dim\n",
    "        self.node_mlp = nn.Sequential(*[nn.Linear(node_in_dim, int(node_dim/2)), nn.ReLU(), nn.Dropout(dropout),\n",
    "                                        nn.Linear(int(node_dim/2), node_dim), nn.ReLU(), nn.Dropout(dropout)])\n",
    "\n",
    "    def edge_update(self, edge_embeds, nodes_a_embeds, nodes_b_embeds):\n",
    "        \"\"\"\n",
    "        Node-to-edge updates, as descibed in slide 71, lecture 5.\n",
    "        Args:\n",
    "            edge_embeds: torch.Tensor with shape (|A|, |B|, 2 x edge_dim)\n",
    "            nodes_a_embeds: torch.Tensor with shape (|A|, node_dim)\n",
    "            nodes_b_embeds: torch.Tensor with shape (|B|, node_dim)\n",
    "\n",
    "        returns:\n",
    "            updated_edge_feats = torch.Tensor with shape (|A|, |B|, edge_dim)\n",
    "        \"\"\"\n",
    "\n",
    "        n_nodes_a, n_nodes_b, _  = edge_embeds.shape\n",
    "        _, node_dim = nodes_a_embeds.shape\n",
    "\n",
    "        ########################\n",
    "        #### TODO starts\n",
    "        ########################\n",
    "\n",
    "        nodes_a_embeds_repeated =  nodes_a_embeds.reshape((n_nodes_a, 1, node_dim)).repeat_interleave(n_nodes_b, axis = 1)  # |A|, |B|, node_dim\n",
    "        nodes_b_embeds_repeated =  nodes_b_embeds.reshape((1, n_nodes_b, node_dim)).repeat_interleave(n_nodes_a, axis = 0)  # |A|, |B|, node_dim\n",
    "        edge_in = torch.cat((nodes_a_embeds_repeated, edge_embeds, nodes_b_embeds_repeated), 2) # |A|, |B|, 2*edge_dim + 2*node_dim\n",
    "\n",
    "        ########################\n",
    "        #### TODO ends\n",
    "        ########################\n",
    "\n",
    "\n",
    "        return self.edge_mlp(edge_in)\n",
    "\n",
    "    def node_update(self, edge_embeds, nodes_a_embeds, nodes_b_embeds):\n",
    "        \"\"\"\n",
    "        Edge-to-node updates, as descibed in slide 75, lecture 5.\n",
    "\n",
    "        Args:\n",
    "            edge_embeds: torch.Tensor with shape (|A|, |B|, edge_dim)\n",
    "            nodes_a_embeds: torch.Tensor with shape (|A|, node_dim)\n",
    "            nodes_b_embeds: torch.Tensor with shape (|B|, node_dim)\n",
    "\n",
    "        returns:\n",
    "            tuple(\n",
    "                updated_nodes_a_embeds: torch.Tensor with shape (|A|, node_dim),\n",
    "                updated_nodes_b_embeds: torch.Tensor with shape (|B|, node_dim)\n",
    "                )\n",
    "        \"\"\"\n",
    "\n",
    "        ########################\n",
    "        #### TODO starts\n",
    "        ########################\n",
    "        nodes_a, nodes_b, edge_dim = edge_embeds.shape\n",
    "\n",
    "        # NOTE: Use 'sum' as aggregation function\n",
    "        a_edges = torch.sum(edge_embeds, 1).reshape((nodes_a, edge_dim))\n",
    "        b_edges = torch.sum(edge_embeds, 0).reshape((nodes_b, edge_dim))\n",
    "\n",
    "        nodes_a_in = torch.cat((nodes_a_embeds, a_edges), 1)\n",
    "        nodes_b_in = torch.cat((nodes_b_embeds, b_edges), 1)\n",
    "\n",
    "        ########################\n",
    "        #### TODO ends\n",
    "        ########################\n",
    "\n",
    "        nodes_a = self.node_mlp(nodes_a_in)\n",
    "        nodes_b = self.node_mlp(nodes_b_in)\n",
    "\n",
    "        return nodes_a, nodes_b\n",
    "\n",
    "    def forward(self, edge_embeds, nodes_a_embeds, nodes_b_embeds):\n",
    "        edge_embeds_latent = self.edge_update(edge_embeds, nodes_a_embeds, nodes_b_embeds)\n",
    "        nodes_a_latent, nodes_b_latent = self.node_update(edge_embeds_latent, nodes_a_embeds, nodes_b_embeds)\n",
    "\n",
    "        return edge_embeds_latent, nodes_a_latent, nodes_b_latent"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### AssignmentSimilarityNet\n",
    "Given reid embedding, box coordinate and timestamp of past track and current frame detections; classify edges."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def euclidean_distance(input1, input2):\n",
    "        \"\"\"Computes l2 distance.\n",
    "        Args:\n",
    "            input1 (torch.Tensor): 2-D feature matrix.\n",
    "            input2 (torch.Tensor): 2-D feature matrix.\n",
    "        Returns:\n",
    "            torch.Tensor: distance matrix.\n",
    "        \"\"\"\n",
    "        input1 = torch.nn.functional.normalize(input1, p=2, dim=1)\n",
    "        input2 = torch.nn.functional.normalize(input2, p=2, dim=1)\n",
    "        input2_t = input2.t()\n",
    "        distmat = torch.zeros(size=(input1.shape[0], input2_t.shape[1])).cuda()\n",
    "        distmat += torch.mm(torch.square(input1), torch.ones(size=input2_t.shape).cuda())\n",
    "        distmat += -2*torch.mm(input1, input2_t)\n",
    "        distmat += torch.mm(torch.ones(size=input1.shape).cuda(), torch.square(input2_t))\n",
    "        return distmat"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "class AssignmentSimilarityNet(nn.Module):\n",
    "    def __init__(self, reid_network, node_dim, edge_dim, reid_dim, edges_in_dim, edges_hidden_dim, num_steps, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.reid_network = reid_network\n",
    "        self.graph_net = BipartiteNeuralMessagePassingLayer(node_dim=node_dim, edge_dim=edge_dim, dropout=dropout)\n",
    "        self.num_steps = num_steps\n",
    "        self.cnn_linear = nn.Sequential(*[nn.Linear(reid_dim, int(reid_dim/4)), nn.ReLU(), nn.Dropout(dropout), nn.Linear(int(reid_dim/4), node_dim), nn.ReLU(),nn.Dropout(dropout)])\n",
    "        self.edge_in_mlp = nn.Sequential(*[nn.Linear(edges_in_dim, edges_hidden_dim), nn.ReLU(), nn.Dropout(dropout), nn.Linear(edges_hidden_dim, edges_hidden_dim), nn.ReLU(),nn.Dropout(dropout), nn.Linear(edges_hidden_dim, edge_dim), nn.ReLU(),nn.Dropout(dropout)])\n",
    "        self.classifier = nn.Sequential(*[nn.Linear(edge_dim, int(edge_dim/2)), nn.ReLU(), nn.Linear(int(edge_dim/2), 1)])\n",
    "\n",
    "\n",
    "    def compute_edge_feats(self, track_coords, current_coords, track_t, curr_t):\n",
    "        \"\"\"\n",
    "        Computes initial edge feature tensor\n",
    "\n",
    "        Args:\n",
    "            track_coords: track's frame box coordinates, given by top-left and bottom-right coordinates\n",
    "                          torch.Tensor with shape (num_tracks, 4)\n",
    "            current_coords: current frame box coordinates, given by top-left and bottom-right coordinates\n",
    "                            has shape (num_boxes, 4)\n",
    "\n",
    "            track_t: track's timestamps, torch.Tensor with with shape (num_tracks, )\n",
    "            curr_t: current frame's timestamps, torch.Tensor withwith shape (num_boxes,)\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            tensor with shape (num_trakcs, num_boxes, 5) containing pairwise\n",
    "            position and time difference features\n",
    "        \"\"\"\n",
    "\n",
    "        ########################\n",
    "        #### TODO starts\n",
    "        ########################\n",
    "        n_a, _ = track_coords.shape\n",
    "        n_b, _ = current_coords.shape\n",
    "\n",
    "        # NOTE 1: we recommend you to use box centers to compute distances\n",
    "        # in the x and y coordinates.\n",
    "\n",
    "        # NOTE 2: Check out the code inside train_one_epoch function and\n",
    "        # LongTrackTrainingDataset class a few cells below to debug this\n",
    "\n",
    "        # convert to shape (num_trakcs, num_boxes, 5)\n",
    "        track_coords_reshaped = track_coords.reshape((n_a, 1, 4)).repeat_interleave(n_b, axis = 1)\n",
    "        current_coords_reshaped = current_coords.reshape((1, n_b, 4)).repeat_interleave(n_a, axis = 0)\n",
    "        track_t_reshaped = track_t.reshape((n_a, 1, 1)).repeat_interleave(n_b, axis = 1)\n",
    "        curr_t_reshaped = curr_t.reshape((1, n_b, 1)).repeat_interleave(n_a, axis = 0)\n",
    "\n",
    "        current_coords_reshaped_h = current_coords_reshaped[:,:,3] - current_coords_reshaped[:,:,1]\n",
    "        current_coords_reshaped_w = current_coords_reshaped[:,:,2] - current_coords_reshaped[:,:,0]\n",
    "        track_coords_reshaped_h = track_coords_reshaped[:,:,3] - track_coords_reshaped[:,:,1]\n",
    "        track_coords_reshaped_w = track_coords_reshaped[:,:,2] - track_coords_reshaped[:,:,0]\n",
    "\n",
    "        edge_feats = torch.zeros((n_a,n_b,5))\n",
    "        edge_feats[:, :, 0] = 2*(current_coords_reshaped[:,:,0]-track_coords_reshaped[:,:,0])/(track_coords_reshaped_h+current_coords_reshaped_h)\n",
    "        edge_feats[:, :, 1] = 2*(current_coords_reshaped[:,:,1]-track_coords_reshaped[:,:,1])/(track_coords_reshaped_h+current_coords_reshaped_h)\n",
    "        edge_feats[:, :, 2] = torch.log(track_coords_reshaped_h/current_coords_reshaped_h)\n",
    "        edge_feats[:, :, 3] = torch.log(track_coords_reshaped_w/current_coords_reshaped_w)\n",
    "        edge_feats[:, :, 4] = curr_t_reshaped[:,:,0]-track_t_reshaped[:,:,0]\n",
    "\n",
    "        ########################\n",
    "        #### TODO ends\n",
    "        ########################\n",
    "\n",
    "        return edge_feats # has shape (num_trakcs, num_boxes, 5)\n",
    "\n",
    "    def forward(self, track_app, current_app, track_coords, current_coords, track_t, curr_t):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            track_app: track's reid embeddings, torch.Tensor with shape (num_tracks, 512)\n",
    "            current_app: current frame detections' reid embeddings, torch.Tensor with shape (num_boxes, 512)\n",
    "            track_coords: track's frame box coordinates, given by top-left and bottom-right coordinates\n",
    "                          torch.Tensor with shape (num_tracks, 4)\n",
    "            current_coords: current frame box coordinates, given by top-left and bottom-right coordinates\n",
    "                            has shape (num_boxes, 4)\n",
    "\n",
    "            track_t: track's timestamps, torch.Tensor with shape (num_tracks, )\n",
    "            curr_t: current frame's timestamps, torch.Tensor with shape (num_boxes,)\n",
    "\n",
    "        Returns:\n",
    "            classified edges: torch.Tensor with shape (num_steps, num_tracks, num_boxes),\n",
    "                             containing at entry (step, i, j) the unnormalized probability that track i and\n",
    "                             detection j are a match, according to the classifier at the given neural message passing step\n",
    "        \"\"\"\n",
    "\n",
    "        # initial edge embeddings\n",
    "        dist_reid = euclidean_distance(track_app, current_app)\n",
    "        pos_edge_feats = self.compute_edge_feats(track_coords, current_coords, track_t, curr_t)\n",
    "        edge_feats = torch.cat((pos_edge_feats.cuda(), dist_reid.unsqueeze(-1)), dim=-1)\n",
    "        edge_embeds = self.edge_in_mlp(edge_feats)\n",
    "        initial_edge_embeds = edge_embeds.clone()\n",
    "\n",
    "        # Get initial node embeddings, reduce dimensionality from 512 to node_dim\n",
    "        track_embeds = F.relu(self.cnn_linear(track_app))\n",
    "        curr_embeds =F.relu(self.cnn_linear(current_app))\n",
    "\n",
    "        classified_edges = []\n",
    "        for _ in range(self.num_steps):\n",
    "            edge_embeds = torch.cat((edge_embeds, initial_edge_embeds), dim=-1)\n",
    "            edge_embeds, track_embeds, curr_embeds = self.graph_net(edge_embeds=edge_embeds,\n",
    "                                                                    nodes_a_embeds=track_embeds,\n",
    "                                                                    nodes_b_embeds=curr_embeds)\n",
    "\n",
    "            classified_edges.append(self.classifier(edge_embeds))\n",
    "\n",
    "        return torch.stack(classified_edges).squeeze(-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from gnn.dataset import LongTrackTrainingDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from gnn.trainer import train_one_epoch\n",
    "\n",
    "MAX_PATIENCE = 20\n",
    "MAX_EPOCHS = 20\n",
    "EVAL_FREQ = 1\n",
    "\n",
    "\n",
    "# Define our model, and init\n",
    "assign_net = AssignmentSimilarityNet(reid_network=None, # Not needed since we work with precomputed features\n",
    "                                     node_dim=32,\n",
    "                                     edge_dim=16,\n",
    "                                     reid_dim=512,\n",
    "                                     edges_in_dim=6,\n",
    "                                     edges_hidden_dim=18,\n",
    "                                     num_steps=15).cuda()\n",
    "\n",
    "# We only keep two sequences for validation. You can\n",
    "dataset = LongTrackTrainingDataset(dataset='MOT16-train',\n",
    "                                   db=train_db,\n",
    "                                   root_dir= osp.join(root_dir, 'data/MOT16'),\n",
    "                                   max_past_frames = MAX_PATIENCE,\n",
    "                                   vis_threshold=0.25)\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=8, collate_fn = lambda x: x,\n",
    "                         shuffle=True, num_workers=0, drop_last=True)\n",
    "device = torch.device('cuda')\n",
    "optimizer = torch.optim.Adam(assign_net.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- EPOCH  1 --------\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "38c8643d79c44b3b94fb7d255016d5ec"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 100. Loss: 1.338. Accuracy: 0.938. Recall: 0.448. Precision: 0.345\n",
      "Iter 200. Loss: 0.163. Accuracy: 0.993. Recall: 0.978. Precision: 0.917\n",
      "Iter 300. Loss: 0.066. Accuracy: 0.996. Recall: 0.992. Precision: 0.946\n",
      "Iter 400. Loss: 0.033. Accuracy: 0.999. Recall: 0.996. Precision: 0.981\n",
      "Iter 500. Loss: 0.025. Accuracy: 0.999. Recall: 0.997. Precision: 0.988\n",
      "Iter 600. Loss: 0.018. Accuracy: 0.999. Recall: 0.997. Precision: 0.989\n",
      "\n",
      "Tracking: MOT16-02\n",
      "Tracks found: 117\n",
      "Runtime for MOT16-02: 15.9 s.\n",
      "Tracking: MOT16-11\n",
      "Tracks found: 97\n",
      "Runtime for MOT16-11: 18.9 s.\n",
      "Runtime for all sequences: 34.8 s.\n",
      "          IDF1   IDP   IDR  Rcll  Prcn  GT MT PT ML  FP    FN IDs   FM  MOTA  MOTP\n",
      "MOT16-02 48.8% 69.2% 37.6% 52.2% 96.1%  62 11 38 13 390  8873 108  216 49.6% 0.094\n",
      "MOT16-11 70.5% 77.7% 64.5% 80.2% 96.6%  75 44 24  7 266  1871  37   89 77.0% 0.083\n",
      "OVERALL  56.9% 72.9% 46.7% 61.7% 96.3% 137 55 62 20 656 10744 145  305 58.8% 0.089\n",
      "-------- EPOCH  2 --------\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a52c55c4fd184d26b05e37bc38bd2222"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 100. Loss: 0.019. Accuracy: 0.999. Recall: 0.997. Precision: 0.990\n",
      "Iter 200. Loss: 0.016. Accuracy: 0.999. Recall: 0.999. Precision: 0.992\n",
      "Iter 300. Loss: 0.013. Accuracy: 0.999. Recall: 0.998. Precision: 0.993\n",
      "Iter 400. Loss: 0.018. Accuracy: 0.999. Recall: 0.998. Precision: 0.990\n",
      "Iter 500. Loss: 0.016. Accuracy: 0.999. Recall: 0.997. Precision: 0.992\n",
      "Iter 600. Loss: 0.013. Accuracy: 0.999. Recall: 0.998. Precision: 0.991\n",
      "\n",
      "Tracking: MOT16-02\n",
      "Tracks found: 111\n",
      "Runtime for MOT16-02: 15.5 s.\n",
      "Tracking: MOT16-11\n",
      "Tracks found: 88\n",
      "Runtime for MOT16-11: 20.0 s.\n",
      "Runtime for all sequences: 35.5 s.\n",
      "          IDF1   IDP   IDR  Rcll  Prcn  GT MT PT ML  FP    FN IDs   FM  MOTA  MOTP\n",
      "MOT16-02 48.5% 68.9% 37.4% 52.2% 96.1%  62 11 38 13 390  8873 102  219 49.6% 0.095\n",
      "MOT16-11 71.6% 78.9% 65.5% 80.2% 96.6%  75 44 24  7 266  1871  34   90 77.0% 0.083\n",
      "OVERALL  57.2% 73.2% 46.9% 61.7% 96.3% 137 55 62 20 656 10744 136  309 58.8% 0.090\n",
      "-------- EPOCH  3 --------\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7aed7322d43d4b8995131f3221c51363"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 100. Loss: 0.013. Accuracy: 0.999. Recall: 0.998. Precision: 0.990\n",
      "Iter 200. Loss: 0.012. Accuracy: 0.999. Recall: 0.998. Precision: 0.988\n",
      "Iter 300. Loss: 0.012. Accuracy: 0.999. Recall: 0.999. Precision: 0.993\n",
      "Iter 400. Loss: 0.011. Accuracy: 0.999. Recall: 0.999. Precision: 0.992\n",
      "Iter 500. Loss: 0.008. Accuracy: 1.000. Recall: 0.999. Precision: 0.994\n",
      "Iter 600. Loss: 0.012. Accuracy: 0.999. Recall: 0.998. Precision: 0.993\n",
      "\n",
      "Tracking: MOT16-02\n",
      "Tracks found: 92\n",
      "Runtime for MOT16-02: 15.8 s.\n",
      "Tracking: MOT16-11\n",
      "Tracks found: 80\n",
      "Runtime for MOT16-11: 18.8 s.\n",
      "Runtime for all sequences: 34.6 s.\n",
      "          IDF1   IDP   IDR  Rcll  Prcn  GT MT PT ML  FP    FN IDs   FM  MOTA  MOTP\n",
      "MOT16-02 45.9% 65.1% 35.4% 52.2% 96.1%  62 11 38 13 390  8873 104  221 49.6% 0.095\n",
      "MOT16-11 71.3% 78.6% 65.2% 80.2% 96.6%  75 44 24  7 266  1871  33   90 77.0% 0.083\n",
      "OVERALL  55.4% 71.0% 45.4% 61.7% 96.3% 137 55 62 20 656 10744 137  311 58.8% 0.090\n",
      "-------- EPOCH  4 --------\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dd33307ba71f4152b67b73f8d40272d6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 100. Loss: 0.012. Accuracy: 1.000. Recall: 0.999. Precision: 0.994\n",
      "Iter 200. Loss: 0.013. Accuracy: 0.999. Recall: 0.999. Precision: 0.992\n",
      "Iter 300. Loss: 0.010. Accuracy: 0.999. Recall: 0.998. Precision: 0.993\n",
      "Iter 400. Loss: 34042.116. Accuracy: 0.995. Recall: 0.956. Precision: 0.948\n",
      "Iter 500. Loss: 1.345. Accuracy: 0.903. Recall: 0.668. Precision: 0.375\n",
      "Iter 600. Loss: 0.511. Accuracy: 0.916. Recall: 0.961. Precision: 0.543\n",
      "\n",
      "Tracking: MOT16-02\n",
      "Tracks found: 53\n",
      "Runtime for MOT16-02: 12.3 s.\n",
      "Tracking: MOT16-11\n",
      "Tracks found: 531\n",
      "Runtime for MOT16-11: 15.0 s.\n",
      "Runtime for all sequences: 27.3 s.\n",
      "          IDF1   IDP   IDR  Rcll  Prcn  GT MT PT ML  FP    FN  IDs   FM  MOTA  MOTP\n",
      "MOT16-02 32.5% 46.2% 25.1% 52.2% 96.1%  62 12 38 12 390  8873 1127  293 44.1% 0.093\n",
      "MOT16-11 27.6% 30.5% 25.3% 80.2% 96.6%  75 44 25  6 266  1871 5142  103 22.9% 0.083\n",
      "OVERALL  30.7% 39.3% 25.2% 61.7% 96.3% 137 56 63 18 656 10744 6269  396 36.9% 0.089\n",
      "-------- EPOCH  5 --------\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "45fa3b6d1f764d20ac69d60231ef0ed4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 100. Loss: 0.258. Accuracy: 0.961. Recall: 0.981. Precision: 0.688\n",
      "Iter 200. Loss: 0.237. Accuracy: 0.963. Recall: 0.984. Precision: 0.709\n",
      "Iter 300. Loss: 0.240. Accuracy: 0.964. Recall: 0.982. Precision: 0.715\n",
      "Iter 400. Loss: 0.194. Accuracy: 0.971. Recall: 0.985. Precision: 0.749\n",
      "Iter 500. Loss: 0.201. Accuracy: 0.974. Recall: 0.985. Precision: 0.777\n",
      "Iter 600. Loss: 0.190. Accuracy: 0.975. Recall: 0.985. Precision: 0.790\n",
      "\n",
      "Tracking: MOT16-02\n",
      "Tracks found: 71\n",
      "Runtime for MOT16-02: 12.9 s.\n",
      "Tracking: MOT16-11\n",
      "Tracks found: 65\n",
      "Runtime for MOT16-11: 15.5 s.\n",
      "Runtime for all sequences: 28.4 s.\n",
      "          IDF1   IDP   IDR  Rcll  Prcn  GT MT PT ML  FP    FN IDs   FM  MOTA  MOTP\n",
      "MOT16-02 36.6% 51.9% 28.2% 52.2% 96.1%  62 12 38 12 390  8873 385  239 48.1% 0.095\n",
      "MOT16-11 59.1% 65.2% 54.1% 80.2% 96.6%  75 44 25  6 267  1872 581   95 71.2% 0.083\n",
      "OVERALL  45.0% 57.7% 36.9% 61.6% 96.3% 137 56 63 18 657 10745 966  334 55.9% 0.090\n",
      "-------- EPOCH  6 --------\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "68316fe7be1946f8b1fe42e854e9aa6d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 100. Loss: 0.189. Accuracy: 0.979. Recall: 0.984. Precision: 0.814\n",
      "Iter 200. Loss: 0.187. Accuracy: 0.979. Recall: 0.987. Precision: 0.815\n",
      "Iter 300. Loss: 0.173. Accuracy: 0.979. Recall: 0.989. Precision: 0.824\n",
      "Iter 400. Loss: 0.190. Accuracy: 0.978. Recall: 0.986. Precision: 0.820\n",
      "Iter 500. Loss: 0.182. Accuracy: 0.980. Recall: 0.986. Precision: 0.822\n",
      "Iter 600. Loss: 0.172. Accuracy: 0.981. Recall: 0.986. Precision: 0.823\n",
      "\n",
      "Tracking: MOT16-02\n",
      "Tracks found: 68\n",
      "Runtime for MOT16-02: 13.3 s.\n",
      "Tracking: MOT16-11\n",
      "Tracks found: 65\n",
      "Runtime for MOT16-11: 17.1 s.\n",
      "Runtime for all sequences: 30.4 s.\n",
      "          IDF1   IDP   IDR  Rcll  Prcn  GT MT PT ML  FP    FN IDs   FM  MOTA  MOTP\n",
      "MOT16-02 35.8% 50.8% 27.6% 52.2% 96.1%  62 12 38 12 390  8873 372  240 48.1% 0.095\n",
      "MOT16-11 60.6% 66.8% 55.4% 80.2% 96.6%  75 44 25  6 267  1872 570   92 71.3% 0.083\n",
      "OVERALL  45.1% 57.8% 37.0% 61.6% 96.3% 137 56 63 18 657 10745 942  332 55.9% 0.090\n",
      "-------- EPOCH  7 --------\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4ed1833d6e684691abae0bcb4a8d5338"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 100. Loss: 0.181. Accuracy: 0.978. Recall: 0.986. Precision: 0.821\n",
      "Iter 200. Loss: 0.176. Accuracy: 0.980. Recall: 0.985. Precision: 0.831\n",
      "Iter 300. Loss: 0.186. Accuracy: 0.980. Recall: 0.983. Precision: 0.826\n",
      "Iter 400. Loss: 0.184. Accuracy: 0.979. Recall: 0.985. Precision: 0.816\n",
      "Iter 500. Loss: 0.174. Accuracy: 0.980. Recall: 0.985. Precision: 0.825\n",
      "Iter 600. Loss: 0.176. Accuracy: 0.980. Recall: 0.988. Precision: 0.827\n",
      "\n",
      "Tracking: MOT16-02\n",
      "Tracks found: 68\n",
      "Runtime for MOT16-02: 12.6 s.\n",
      "Tracking: MOT16-11\n",
      "Tracks found: 66\n",
      "Runtime for MOT16-11: 15.6 s.\n",
      "Runtime for all sequences: 28.2 s.\n",
      "          IDF1   IDP   IDR  Rcll  Prcn  GT MT PT ML  FP    FN IDs   FM  MOTA  MOTP\n",
      "MOT16-02 35.9% 51.0% 27.7% 52.2% 96.1%  62 12 38 12 390  8873 311  239 48.5% 0.095\n",
      "MOT16-11 62.0% 68.4% 56.8% 80.2% 96.6%  75 44 25  6 267  1872 534   92 71.7% 0.083\n",
      "OVERALL  45.7% 58.6% 37.5% 61.6% 96.3% 137 56 63 18 657 10745 845  331 56.3% 0.090\n",
      "-------- EPOCH  8 --------\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e9c548fb163648479cf7160b1cdd837c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 100. Loss: 0.176. Accuracy: 0.981. Recall: 0.986. Precision: 0.826\n",
      "Iter 200. Loss: 0.169. Accuracy: 0.982. Recall: 0.985. Precision: 0.836\n",
      "Iter 300. Loss: 0.173. Accuracy: 0.980. Recall: 0.986. Precision: 0.829\n",
      "Iter 400. Loss: 0.174. Accuracy: 0.981. Recall: 0.987. Precision: 0.828\n",
      "Iter 500. Loss: 0.180. Accuracy: 0.980. Recall: 0.983. Precision: 0.830\n",
      "Iter 600. Loss: 0.170. Accuracy: 0.982. Recall: 0.989. Precision: 0.835\n",
      "\n",
      "Tracking: MOT16-02\n",
      "Tracks found: 70\n",
      "Runtime for MOT16-02: 14.1 s.\n",
      "Tracking: MOT16-11\n",
      "Tracks found: 67\n",
      "Runtime for MOT16-11: 15.1 s.\n",
      "Runtime for all sequences: 29.2 s.\n",
      "          IDF1   IDP   IDR  Rcll  Prcn  GT MT PT ML  FP    FN IDs   FM  MOTA  MOTP\n",
      "MOT16-02 36.9% 52.4% 28.5% 52.2% 96.1%  62 12 38 12 390  8873 301  241 48.5% 0.096\n",
      "MOT16-11 62.7% 69.1% 57.3% 80.2% 96.6%  75 44 25  6 267  1872 505   92 72.0% 0.083\n",
      "OVERALL  46.6% 59.7% 38.2% 61.6% 96.3% 137 56 63 18 657 10745 806  333 56.4% 0.090\n",
      "-------- EPOCH  9 --------\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "05defe7b9ec848a0b239969f7d3d1a4f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 100. Loss: 0.168. Accuracy: 0.981. Recall: 0.988. Precision: 0.835\n",
      "Iter 200. Loss: 0.164. Accuracy: 0.982. Recall: 0.987. Precision: 0.835\n",
      "Iter 300. Loss: 0.175. Accuracy: 0.981. Recall: 0.987. Precision: 0.833\n",
      "Iter 400. Loss: 0.181. Accuracy: 0.980. Recall: 0.987. Precision: 0.832\n",
      "Iter 500. Loss: 0.164. Accuracy: 0.983. Recall: 0.989. Precision: 0.838\n",
      "Iter 600. Loss: 0.171. Accuracy: 0.980. Recall: 0.987. Precision: 0.830\n",
      "\n",
      "Tracking: MOT16-02\n",
      "Tracks found: 74\n",
      "Runtime for MOT16-02: 12.4 s.\n",
      "Tracking: MOT16-11\n",
      "Tracks found: 68\n",
      "Runtime for MOT16-11: 14.6 s.\n",
      "Runtime for all sequences: 27.0 s.\n",
      "          IDF1   IDP   IDR  Rcll  Prcn  GT MT PT ML  FP    FN IDs   FM  MOTA  MOTP\n",
      "MOT16-02 36.5% 51.8% 28.1% 52.2% 96.1%  62 12 38 12 390  8873 304  240 48.5% 0.096\n",
      "MOT16-11 63.9% 70.5% 58.5% 80.2% 96.6%  75 44 25  6 267  1872 482   92 72.2% 0.083\n",
      "OVERALL  46.8% 59.9% 38.4% 61.6% 96.3% 137 56 63 18 657 10745 786  332 56.5% 0.090\n",
      "-------- EPOCH 10 --------\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8d2322c446c64ab68bb0183d06531d77"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 100. Loss: 0.168. Accuracy: 0.981. Recall: 0.988. Precision: 0.838\n",
      "Iter 200. Loss: 0.167. Accuracy: 0.982. Recall: 0.991. Precision: 0.840\n",
      "Iter 300. Loss: 0.183. Accuracy: 0.979. Recall: 0.986. Precision: 0.831\n",
      "Iter 400. Loss: 0.164. Accuracy: 0.983. Recall: 0.986. Precision: 0.844\n",
      "Iter 500. Loss: 0.166. Accuracy: 0.982. Recall: 0.988. Precision: 0.838\n",
      "Iter 600. Loss: 0.170. Accuracy: 0.982. Recall: 0.986. Precision: 0.842\n",
      "\n",
      "Tracking: MOT16-02\n",
      "Tracks found: 73\n",
      "Runtime for MOT16-02: 12.5 s.\n",
      "Tracking: MOT16-11\n",
      "Tracks found: 68\n",
      "Runtime for MOT16-11: 15.6 s.\n",
      "Runtime for all sequences: 28.1 s.\n",
      "          IDF1   IDP   IDR  Rcll  Prcn  GT MT PT ML  FP    FN IDs   FM  MOTA  MOTP\n",
      "MOT16-02 37.0% 52.5% 28.5% 52.2% 96.1%  62 12 38 12 390  8873 303  239 48.5% 0.095\n",
      "MOT16-11 63.4% 69.9% 58.0% 80.2% 96.6%  75 44 24  7 267  1872 462   91 72.4% 0.083\n",
      "OVERALL  46.9% 60.1% 38.4% 61.6% 96.3% 137 56 62 19 657 10745 765  330 56.6% 0.090\n",
      "-------- EPOCH 11 --------\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "11ebef62edb64376ad31694bdd2e8933"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 100. Loss: 0.155. Accuracy: 0.984. Recall: 0.988. Precision: 0.842\n",
      "Iter 200. Loss: 0.171. Accuracy: 0.981. Recall: 0.989. Precision: 0.835\n",
      "Iter 300. Loss: 0.180. Accuracy: 0.981. Recall: 0.984. Precision: 0.830\n",
      "Iter 400. Loss: 0.167. Accuracy: 0.983. Recall: 0.986. Precision: 0.839\n",
      "Iter 500. Loss: 0.179. Accuracy: 0.979. Recall: 0.987. Precision: 0.838\n",
      "Iter 600. Loss: 0.170. Accuracy: 0.981. Recall: 0.987. Precision: 0.840\n",
      "\n",
      "Tracking: MOT16-02\n",
      "Tracks found: 73\n",
      "Runtime for MOT16-02: 12.0 s.\n",
      "Tracking: MOT16-11\n",
      "Tracks found: 67\n",
      "Runtime for MOT16-11: 14.7 s.\n",
      "Runtime for all sequences: 26.7 s.\n",
      "          IDF1   IDP   IDR  Rcll  Prcn  GT MT PT ML  FP    FN IDs   FM  MOTA  MOTP\n",
      "MOT16-02 37.1% 52.6% 28.6% 52.2% 96.1%  62 12 38 12 390  8873 302  239 48.5% 0.095\n",
      "MOT16-11 63.0% 69.4% 57.6% 80.2% 96.6%  75 44 24  7 267  1872 459   91 72.5% 0.083\n",
      "OVERALL  46.8% 60.0% 38.4% 61.6% 96.3% 137 56 62 19 657 10745 761  330 56.6% 0.090\n",
      "-------- EPOCH 12 --------\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7126ae113fb345e68229bec31b067f47"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 100. Loss: 0.181. Accuracy: 0.980. Recall: 0.986. Precision: 0.830\n",
      "Iter 200. Loss: 0.167. Accuracy: 0.981. Recall: 0.988. Precision: 0.836\n",
      "Iter 300. Loss: 0.167. Accuracy: 0.981. Recall: 0.986. Precision: 0.843\n",
      "Iter 400. Loss: 0.165. Accuracy: 0.984. Recall: 0.987. Precision: 0.843\n",
      "Iter 500. Loss: 0.165. Accuracy: 0.982. Recall: 0.988. Precision: 0.833\n",
      "Iter 600. Loss: 0.177. Accuracy: 0.979. Recall: 0.987. Precision: 0.831\n",
      "\n",
      "Tracking: MOT16-02\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_9932/3182485402.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      8\u001B[0m         \u001B[0mtracker\u001B[0m \u001B[1;33m=\u001B[0m  \u001B[0mMPNTracker\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0massign_net\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0massign_net\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0meval\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mobj_detect\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpatience\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mMAX_PATIENCE\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m         \u001B[0mval_sequences\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mMOT16Sequences\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'MOT16-val2'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mosp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mroot_dir\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'data/MOT16'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvis_threshold\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0.\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 10\u001B[1;33m         \u001B[0mres\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mrun_tracker\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mval_sequences\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdb\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtrain_db\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtracker\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtracker\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moutput_dir\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     11\u001B[0m         \u001B[0midf1\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mres\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mloc\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'OVERALL'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'idf1'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0midf1\u001B[0m \u001B[1;33m>\u001B[0m \u001B[0mbest_idf1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\TUM\\CV3\\cv3dst_gnn_exercise\\src\\tracker\\utils.py\u001B[0m in \u001B[0;36mrun_tracker\u001B[1;34m(val_sequences, db, tracker, output_dir)\u001B[0m\n\u001B[0;32m    258\u001B[0m             \u001B[1;31m#for i, frame in enumerate(tqdm(data_loader)):\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    259\u001B[0m             \u001B[1;32mfor\u001B[0m \u001B[0mframe\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mdb\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mseq\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 260\u001B[1;33m                 \u001B[0mtracker\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mframe\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    261\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    262\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\TUM\\CV3\\cv3dst_gnn_exercise\\src\\tracker\\tracker.py\u001B[0m in \u001B[0;36mstep\u001B[1;34m(self, frame)\u001B[0m\n\u001B[0;32m    120\u001B[0m                 \u001B[0mscores\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mframe\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'det'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'scores'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    121\u001B[0m                 \u001B[0mreid_feats\u001B[0m\u001B[1;33m=\u001B[0m \u001B[0mframe\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'det'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'reid'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcpu\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 122\u001B[1;33m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata_association\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mboxes\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mscores\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreid_feats\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    123\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    124\u001B[0m                 \u001B[1;31m# results\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_9932/2367306577.py\u001B[0m in \u001B[0;36mdata_association\u001B[1;34m(self, boxes, scores, pred_features)\u001B[0m\n\u001B[0;32m     24\u001B[0m             \u001B[1;31m# Make sure to apply the sigmoid function to them!\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     25\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 26\u001B[1;33m             \u001B[0mpred_sim\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0massign_net\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrack_features\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpred_features\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcuda\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrack_boxes\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mboxes\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcuda\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrack_t\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcurr_t\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     27\u001B[0m             \u001B[0mpred_sim\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msigmoid\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpred_sim\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     28\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Program Files\\Conda\\envs\\i2dl\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m    725\u001B[0m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    726\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 727\u001B[1;33m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    728\u001B[0m         for hook in itertools.chain(\n\u001B[0;32m    729\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_9932/103421054.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, track_app, current_app, track_coords, current_coords, track_t, curr_t)\u001B[0m\n\u001B[0;32m    102\u001B[0m             edge_embeds, track_embeds, curr_embeds = self.graph_net(edge_embeds=edge_embeds,\n\u001B[0;32m    103\u001B[0m                                                                     \u001B[0mnodes_a_embeds\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtrack_embeds\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 104\u001B[1;33m                                                                     nodes_b_embeds=curr_embeds)\n\u001B[0m\u001B[0;32m    105\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    106\u001B[0m             \u001B[0mclassified_edges\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mclassifier\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0medge_embeds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Program Files\\Conda\\envs\\i2dl\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m    725\u001B[0m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    726\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 727\u001B[1;33m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    728\u001B[0m         for hook in itertools.chain(\n\u001B[0;32m    729\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_9932/2818587792.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, edge_embeds, nodes_a_embeds, nodes_b_embeds)\u001B[0m\n\u001B[0;32m     81\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     82\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0medge_embeds\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnodes_a_embeds\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnodes_b_embeds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 83\u001B[1;33m         \u001B[0medge_embeds_latent\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0medge_update\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0medge_embeds\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnodes_a_embeds\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnodes_b_embeds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     84\u001B[0m         \u001B[0mnodes_a_latent\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnodes_b_latent\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnode_update\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0medge_embeds_latent\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnodes_a_embeds\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnodes_b_embeds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     85\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_9932/2818587792.py\u001B[0m in \u001B[0;36medge_update\u001B[1;34m(self, edge_embeds, nodes_a_embeds, nodes_b_embeds)\u001B[0m\n\u001B[0;32m     33\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     34\u001B[0m         \u001B[0mnodes_a_embeds_repeated\u001B[0m \u001B[1;33m=\u001B[0m  \u001B[0mnodes_a_embeds\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mn_nodes_a\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnode_dim\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrepeat_interleave\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mn_nodes_b\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# |A|, |B|, node_dim\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 35\u001B[1;33m         \u001B[0mnodes_b_embeds_repeated\u001B[0m \u001B[1;33m=\u001B[0m  \u001B[0mnodes_b_embeds\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mn_nodes_b\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnode_dim\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrepeat_interleave\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mn_nodes_a\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# |A|, |B|, node_dim\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     36\u001B[0m         \u001B[0medge_in\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnodes_a_embeds_repeated\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0medge_embeds\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnodes_b_embeds_repeated\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m2\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;31m# |A|, |B|, 2*edge_dim + 2*node_dim\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     37\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "best_idf1 = 0.\n",
    "for epoch in range(1, MAX_EPOCHS + 1):\n",
    "    print(f\"-------- EPOCH {epoch:2d} --------\")\n",
    "    train_one_epoch(model = assign_net, data_loader=data_loader, optimizer=optimizer, print_freq=100)\n",
    "    scheduler.step()\n",
    "\n",
    "    if epoch % EVAL_FREQ == 0:\n",
    "        tracker =  MPNTracker(assign_net=assign_net.eval(), obj_detect=None, patience=MAX_PATIENCE)\n",
    "        val_sequences = MOT16Sequences('MOT16-val2', osp.join(root_dir, 'data/MOT16'), vis_threshold=0.)\n",
    "        res = run_tracker(val_sequences, db=train_db, tracker=tracker, output_dir=None)\n",
    "        idf1 = res.loc['OVERALL']['idf1']\n",
    "        if idf1 > best_idf1:\n",
    "            best_idf1 = idf1\n",
    "            torch.save(assign_net.state_dict(), osp.join(root_dir, 'output', 'best_ckpt.pth'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "best_ckpt = torch.load(osp.join(root_dir, 'output', 'best_ckpt.pth'))\n",
    "assign_net.load_state_dict(best_ckpt)\n",
    "tracker =  MPNTracker(assign_net=assign_net.eval(), obj_detect=None, patience=MAX_PATIENCE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Checking Performance\n",
    "Save images with gt and tracking result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "from cycler import cycler as cy\n",
    "cyl = cy('ec', colors)\n",
    "loop_cy_iter = cyl()\n",
    "styles = defaultdict(lambda: next(loop_cy_iter))\n",
    "def plotSequences(seq_name, show_gt=True, show_prediction=False, tracker=None, db=None):\n",
    "    data_dir = os.path.join(root_dir, 'data/MOT16')\n",
    "    sequences = MOT16Sequences(seq_name, data_dir, load_seg=True)\n",
    "    for seq in sequences:\n",
    "        results = None\n",
    "        if show_prediction and tracker is not None:\n",
    "            tracker.reset()\n",
    "            with torch.no_grad():\n",
    "                for frame in db[str(seq)]:\n",
    "                    tracker.step(frame)\n",
    "            results = tracker.get_results()\n",
    "        for i, frame in enumerate(seq):\n",
    "            img = frame['img']\n",
    "\n",
    "            dpi = 150\n",
    "            fig, ax = plt.subplots(1, dpi=dpi)\n",
    "\n",
    "            img = img.mul(255).permute(1, 2, 0).byte().numpy()\n",
    "            width, height, _ = img.shape\n",
    "\n",
    "            ax.imshow(img, cmap='gray')\n",
    "            fig.set_size_inches(width / dpi, height / dpi)\n",
    "\n",
    "            # plot gt\n",
    "            if show_gt and 'gt' in frame:\n",
    "                gt = frame['gt']\n",
    "                for gt_id, box in gt.items():\n",
    "                    rect = plt.Rectangle(\n",
    "                      (box[0], box[1]),\n",
    "                      box[2] - box[0],\n",
    "                      box[3] - box[1],\n",
    "                      fill=False,\n",
    "                      linewidth=1.0)\n",
    "                    ax.add_patch(rect)\n",
    "\n",
    "            # tracker\n",
    "            if results:\n",
    "                for j, t in results.items():\n",
    "                    if i in t.keys():\n",
    "                        t_i = t[i]\n",
    "                        ax.add_patch(\n",
    "                            plt.Rectangle(\n",
    "                                (t_i[0], t_i[1]),\n",
    "                                t_i[2] - t_i[0],\n",
    "                                t_i[3] - t_i[1],\n",
    "                                fill=False,\n",
    "                                linewidth=1.0, **styles[j]\n",
    "                            ))\n",
    "                        ax.annotate(j, (t_i[0] + (t_i[2] - t_i[0]) / 2.0, t_i[1] + (t_i[3] - t_i[1]) / 2.0),\n",
    "                                    color=styles[j]['ec'], weight='bold', fontsize=6, ha='center', va='center')\n",
    "\n",
    "            plt.axis('off')\n",
    "            plt.savefig(\"./VideoOutput/{}\".format(str(i).zfill(4)))\n",
    "            plt.close(fig)\n",
    "            plt.cla()\n",
    "            plt.clf()\n",
    "            plt.close('all')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\CPUAllocator.cpp:73] data. DefaultCPUAllocator: not enough memory: you tried to allocate 24883200 bytes. Buy new RAM!",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_3440/642071169.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m# plotSequences(seq_name='MOT16-02', show_gt=True, show_prediction=True, tracker=tracker, db=train_db)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mplotSequences\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mseq_name\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'MOT16-test'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mshow_gt\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mshow_prediction\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtracker\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtracker\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdb\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtest_db\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_3440/3167264602.py\u001B[0m in \u001B[0;36mplotSequences\u001B[1;34m(seq_name, show_gt, show_prediction, tracker, db)\u001B[0m\n\u001B[0;32m     16\u001B[0m                     \u001B[0mtracker\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mframe\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     17\u001B[0m             \u001B[0mresults\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtracker\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_results\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 18\u001B[1;33m         \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mframe\u001B[0m \u001B[1;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mseq\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     19\u001B[0m             \u001B[0mimg\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mframe\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'img'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\TUM\\CV3\\cv3dst_gnn_exercise\\src\\tracker\\data_track.py\u001B[0m in \u001B[0;36m__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m    141\u001B[0m         \u001B[0mimg\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mImage\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'im_path'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconvert\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"RGB\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    142\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 143\u001B[1;33m         \u001B[0mimg\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtransforms\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimg\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    144\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    145\u001B[0m         \u001B[0msample\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m{\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Program Files\\Conda\\envs\\i2dl\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, pic)\u001B[0m\n\u001B[0;32m    102\u001B[0m             \u001B[0mTensor\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mConverted\u001B[0m \u001B[0mimage\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    103\u001B[0m         \"\"\"\n\u001B[1;32m--> 104\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mF\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto_tensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpic\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    105\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    106\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__repr__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Program Files\\Conda\\envs\\i2dl\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001B[0m in \u001B[0;36mto_tensor\u001B[1;34m(pic)\u001B[0m\n\u001B[0;32m    100\u001B[0m     \u001B[0mimg\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mimg\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpermute\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcontiguous\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    101\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimg\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mByteTensor\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 102\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mimg\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfloat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdiv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m255\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    103\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    104\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mimg\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: [enforce fail at ..\\c10\\core\\CPUAllocator.cpp:73] data. DefaultCPUAllocator: not enough memory: you tried to allocate 24883200 bytes. Buy new RAM!"
     ]
    }
   ],
   "source": [
    "# plotSequences(seq_name='MOT16-02', show_gt=True, show_prediction=True, tracker=tracker, db=train_db)\n",
    "plotSequences(seq_name='MOT16-test', show_gt=True, show_prediction=True, tracker=tracker, db=test_db)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test Output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking: MOT16-01\n",
      "No GT evaluation data available.\n",
      "Tracks found: 74\n",
      "Runtime for MOT16-01: 6.9 s.\n",
      "Writing predictions to: ./cv3dst_exercise/output\\MOT16-01.txt\n",
      "Tracking: MOT16-08\n",
      "No GT evaluation data available.\n",
      "Tracks found: 134\n",
      "Runtime for MOT16-08: 9.7 s.\n",
      "Writing predictions to: ./cv3dst_exercise/output\\MOT16-08.txt\n",
      "Tracking: MOT16-12\n",
      "No GT evaluation data available.\n",
      "Tracks found: 138\n",
      "Runtime for MOT16-12: 13.4 s.\n",
      "Writing predictions to: ./cv3dst_exercise/output\\MOT16-12.txt\n",
      "Runtime for all sequences: 29.9 s.\n"
     ]
    }
   ],
   "source": [
    "val_sequences = MOT16Sequences('MOT16-test', osp.join(root_dir, 'data/MOT16'), vis_threshold=0.)\n",
    "run_tracker(val_sequences, db=test_db, tracker=tracker, output_dir=osp.join(root_dir, 'output'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}