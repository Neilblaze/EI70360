# Top Research Papers by Month

## January 2024

1. [Astraios: Parameter-Efficient Instruction Tuning Code Large Language Models](https://arxiv.org/abs/2401.00788)
2. [A Comprehensive Study of Knowledge Editing for Large Language Models](https://arxiv.org/abs/2401.01286)
3. [LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning](https://arxiv.org/abs/2401.01325)
4. [Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models](https://arxiv.org/abs/2401.01335)
5. [LLaMA Beyond English: An Empirical Study on Language Capability Transfer](https://arxiv.org/abs/2401.01055)
6. [A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity](https://arxiv.org/abs/2401.01967)
7. [LLaMA Pro: Progressive LLaMA with Block Expansion](https://arxiv.org/abs/2401.02415)
8. [LLM Augmented LLMs: Expanding Capabilities through Composition](https://arxiv.org/abs/2401.02412)
9. [Blending Is All You Need: Cheaper, Better Alternative to Trillion-Parameters LLM](https://arxiv.org/abs/2401.02994)
10. [DeepSeek LLM: Scaling Open-Source Language Models with Longtermism](https://arxiv.org/abs/2401.02954)
11. [Denoising Vision Transformers](https://arxiv.org/abs/2401.02957)
12. [Soaring from 4K to 400K: Extending LLMâ€™s Context with Activation Beacon](https://arxiv.org/abs/2401.03462)
13. [Mixtral of Experts](https://arxiv.org/abs/2401.04088)
14. [MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts](https://arxiv.org/abs/2401.04081)
15. [A Minimaximalist Approach to Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2401.04056)
16. [RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation](https://arxiv.org/abs/2401.04679)
17. [Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training](https://arxiv.org/abs/2401.05566)
18. [Transformers are Multi-State RNNs](https://arxiv.org/abs/2401.06104)
19. [A Closer Look at AUROC and AUPRC under Class Imbalance](https://arxiv.org/abs/2401.06091)
20. [An Experimental Design Framework for Label-Efficient Supervised Finetuning of Large Language Models](https://arxiv.org/abs/2401.06692)
21. [Tuning Language Models by Proxy](https://arxiv.org/abs/2401.08565)
22. [Scalable Pre-training of Large Autoregressive Image Models](https://arxiv.org/abs/2401.08541)
23. [Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering](https://arxiv.org/abs/2401.08500)
24. [RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture](https://arxiv.org/abs/2401.08406)
25. [ReFT: Reasoning with Reinforced Fine-Tuning](https://arxiv.org/abs/2401.08967)
26. [DiffusionGPT: LLM-Driven Text-to-Image Generation System](https://arxiv.org/abs/2401.10061)
27. [Self-Rewarding Language Models](https://arxiv.org/abs/2401.10020)
28. [VMamba: Visual State Space Model](https://arxiv.org/abs/2401.10166)
29. [Knowledge Fusion of Large Language Models](https://arxiv.org/abs/2401.10491)
30. [SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities](https://arxiv.org/abs/2401.12168)
31. [WARM: On the Benefits of Weight Averaged Reward Models](https://arxiv.org/abs/2401.12187)
32. [Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated Text](https://arxiv.org/abs/2401.12070)
33. [MambaByte: Token-free Selective State Space Model](https://arxiv.org/abs/2401.13660)
34. [SpacTor-T5: Pre-training T5 Models with Span Corruption and Replaced Token Detection](https://arxiv.org/abs/2401.13160)
35. [Rethinking Patch Dependence for Masked Autoencoders](https://arxiv.org/abs/2401.14391)
36. [Pix2gestalt: Amodal Segmentation by Synthesizing Wholes](https://arxiv.org/abs/2401.14398)
37. [Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities](https://arxiv.org/abs/2401.14405)
38. [EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty](https://arxiv.org/abs/2401.15077)
39. [MoE-LLaVA: Mixture of Experts for Large Vision-Language Models](https://arxiv.org/abs/2401.15947)
40. [Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling](https://arxiv.org/abs/2401.16380)
41. [KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization](https://arxiv.org/abs/2401.18079)

## February 2024

1. [Efficient Exploration for LLMs](https://arxiv.org/abs/2402.00396)
2. [OLMo: Accelerating the Science of Language Models](https://arxiv.org/abs/2402.00838)
3. [Tiny Titans: Can Smaller Large Language Models Punch Above Their Weight in the Real World for Meeting Summarization?](https://arxiv.org/abs/2402.00841)
4. [Repeat After Me: Transformers are Better than State Space Models at Copying](https://arxiv.org/abs/2402.01032)
5. [LiPO: Listwise Preference Optimization through Learning-to-Rank](https://arxiv.org/abs/2402.01878)
6. [FindingEmo: An Image Dataset for Emotion Recognition in the Wild](https://arxiv.org/abs/2402.01355)
7. [More Agents Is All You Need](https://arxiv.org/abs/2402.05120)
8. [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://arxiv.org/abs/2402.03300)
9. [MobileVLM V2: Faster and Stronger Baseline for Vision Language Model](https://arxiv.org/abs/2402.03766)
10. [A Phase Transition Between Positional and Semantic Learning in a Solvable Model of Dot-Product Attention](https://arxiv.org/abs/2402.03902)
11. [Scaling Laws for Downstream Task Performance of Large Language Models](https://arxiv.org/abs/2402.04177)
12. [MOMENT: A Family of Open Time-series Foundation Models](https://arxiv.org/abs/2402.03885)
13. [Vision Superalignment: Weak-to-Strong Generalization for Vision Foundation Models](https://arxiv.org/abs/2402.03749)
14. [Self-Discover: Large Language Models Self-Compose Reasoning Structures](https://arxiv.org/abs/2402.03620)
15. [Grandmaster-Level Chess Without Search](https://arxiv.org/abs/2402.04494)
16. [Direct Language Model Alignment from Online AI Feedback](https://arxiv.org/abs/2402.04792)
17. [Buffer Overflow in Mixture of Experts](https://arxiv.org/abs/2402.05526)
18. [The Boundary of Neural Network Trainability is Fractal](https://arxiv.org/abs/2402.06184)
19. [ODIN: Disentangled Reward Mitigates Hacking in RLHF](https://arxiv.org/abs/2402.07319)
20. [Policy Improvement using Language Feedback Models](https://arxiv.org/abs/2402.07876)
21. [Scaling Laws for Fine-Grained Mixture of Experts](https://arxiv.org/abs/2402.07871)
22. [Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model](https://arxiv.org/abs/2402.07610)
23. [Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping](https://arxiv.org/abs/2402.07610)
24. [Suppressing Pink Elephants with Direct Principle Feedback](https://arxiv.org/abs/2402.07896)
25. [World Model on Million-Length Video And Language With RingAttention](https://arxiv.org/abs/2402.08268)
26. [Mixtures of Experts Unlock Parameter Scaling for Deep RL](https://arxiv.org/abs/2402.08609)
27. [DoRA: Weight-Decomposed Low-Rank Adaptation](https://arxiv.org/abs/2402.09353)
28. [Transformers Can Achieve Length Generalization But Not Robustly](https://arxiv.org/abs/2402.09371)
29. [BASE TTS: Lessons From Building a Billion-Parameter Text-to-Speech Model on 100K Hours of Data](https://arxiv.org/abs/2402.08093)
30. [Recovering the Pre-Fine-Tuning Weights of Generative Models](https://arxiv.org/abs/2402.10208)
31. [Generative Representational Instruction Tuning](https://arxiv.org/abs/2402.09906)
32. [FinTral: A Family of GPT-4 Level Multimodal Financial Large Language Models](https://arxiv.org/abs/2402.10986)
33. [OneBit: Towards Extremely Low-bit Large Language Models](https://arxiv.org/abs/2402.11295)
34. [LongAgent: Scaling Language Models to 128k Context through Multi-Agent Collaboration](https://arxiv.org/abs/2402.11550)
35. [Reformatted Alignment](https://arxiv.org/abs/2402.12219)
36. [AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling](https://arxiv.org/abs/2402.12226)
37. [Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs](https://arxiv.org/abs/2402.12030)
38. [LoRA+: Efficient Low Rank Adaptation of Large Models](https://arxiv.org/abs/2402.12354)
39. [Neural Network Diffusion](https://arxiv.org/abs/2402.13144)
40. [YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information](https://arxiv.org/abs/2402.13616)
41. [LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens](https://arxiv.org/abs/2402.13753)
42. [Large Language Models for Data Annotation: A Survey](https://arxiv.org/abs/2402.13446)
43. [TinyLLaVA: A Framework of Small-scale Large Multimodal Models](https://arxiv.org/abs/2402.14289)
44. [Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs](https://arxiv.org/abs/2402.14740)
45. [Genie: Generative Interactive Environments](https://arxiv.org/abs/2402.15391)
46. [CARTE: Pretraining and Transfer for Tabular Learning](https://arxiv.org/abs/2402.16785)
47. [The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://arxiv.org/abs/2402.17764)
48. [Sora Generates Videos with Stunning Geometrical Consistency](https://arxiv.org/abs/2402.17403)
49. [When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method](https://arxiv.org/abs/2402.17193)
50. [Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models](https://arxiv.org/abs/2402.19427)

## March 2024

1. [Learning and Leveraging World Models in Visual Representation Learning](https://arxiv.org/abs/2403.00504)
2. [Improving LLM Code Generation with Grammar Augmentation](https://arxiv.org/abs/2403.01632)
3. [The Hidden Attention of Mamba Models](https://arxiv.org/abs/2403.01590)
4. [Training-Free Pretrained Model Merging](https://arxiv.org/abs/2403.01753)
5. [Vision-RWKV: Efficient and Scalable Visual Perception with RWKV-Like Architectures](https://arxiv.org/abs/2403.02308)
6. [The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning](https://arxiv.org/abs/2403.03218)
7. [Evolution Transformer: In-Context Evolutionary Optimization](https://arxiv.org/abs/2403.02985)
8. [Enhancing Vision-Language Pre-training with Rich Supervisions](https://arxiv.org/abs/2403.03346)
9. [Scaling Rectified Flow Transformers for High-Resolution Image Synthesis](https://arxiv.org/abs/2403.03206)
10. [Design2Code: How Far Are We From Automating Front-End Engineering?](https://arxiv.org/abs/2403.03163)
11. [ShortGPT: Layers in Large Language Models are More Redundant Than You Expect](https://arxiv.org/abs/2403.03853)
12. [Backtracing: Retrieving the Cause of the Query](https://arxiv.org/abs/2403.03956)
13. [Learning to Decode Collaboratively with Multiple Language Models](https://arxiv.org/abs/2403.03870)
14. [SaulLM-7B: A pioneering Large Language Model for Law](https://arxiv.org/abs/2403.03883)
15. [Are Language Models Puzzle Prodigies? Algorithmic Puzzles Unveil Serious Challenges in Multimodal Reasoning](https://arxiv.org/abs/2403.03864)
16. [3D Diffusion Policy](https://arxiv.org/abs/2403.03954)
17. [MedMamba: Vision Mamba for Medical Image Classification](https://arxiv.org/abs/2403.03849)
18. [GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection](https://arxiv.org/abs/2403.03507)
19. [Stop Regressing: Training Value Functions via Classification for Scalable Deep RL](https://arxiv.org/abs/2403.03950)
20. [How Far Are We from Intelligent Visual Deductive Reasoning?](https://arxiv.org/abs/2403.04732)
21. [Common 7B Language Models Already Possess Strong Math Capabilities](https://arxiv.org/abs/2403.04706)
22. [Gemini 1.5: Unlocking Multimodal Understanding Across Millions of Tokens of Context](https://arxiv.org/abs/2403.05530)
23. [Is Cosine-Similarity of Embeddings Really About Similarity?](https://arxiv.org/abs/2403.05440)
24. [LLM4Decompile: Decompiling Binary Code with Large Language Models](https://arxiv.org/abs/2403.05286)
25. [Algorithmic Progress in Language Models](https://arxiv.org/abs/2403.05812)
26. [Stealing Part of a Production Language Model](https://arxiv.org/abs/2403.06634)
27. [Chronos: Learning the Language of Time Series](https://arxiv.org/abs/2403.07815)
28. [Simple and Scalable Strategies to Continually Pre-train Large Language Models](https://arxiv.org/abs/2403.08763)
29. [Language Models Scale Reliably With Over-Training and on Downstream Tasks](https://arxiv.org/abs/2403.08540)
30. [BurstAttention: An Efficient Distributed Attention Framework for Extremely Long Sequences](https://arxiv.org/abs/2403.09347)
31. [LocalMamba: Visual State Space Model with Windowed Selective Scan](https://arxiv.org/abs/2403.09338)
32. [GiT: Towards Generalist Vision Transformer through Universal Language Interface](https://arxiv.org/abs/2403.09394)
33. [MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training](https://arxiv.org/abs/2403.09611)
34. [RAFT: Adapting Language Model to Domain Specific RAG](https://arxiv.org/abs/2403.10131)
35. [TnT-LLM: Text Mining at Scale with Large Language Models](https://arxiv.org/abs/2403.12173)
36. [Decoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient LLMs Under Compression](https://arxiv.org/abs/2403.15447)
37. [PERL: Parameter Efficient Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2403.10704)
38. [RewardBench: Evaluating Reward Models for Language Modeling](https://arxiv.org/abs/2403.13787)
39. [LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models](https://arxiv.org/abs/2403.13372)
40. [RakutenAI-7B: Extending Large Language Models for Japanese](https://arxiv.org/abs/2403.15484)
41. [SiMBA: Simplified Mamba-Based Architecture for Vision and Multivariate Time Series](https://arxiv.org/abs/2403.15360)
42. [Can Large Language Models Explore In-Context?](https://arxiv.org/abs/2403.15371)
43. [LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement](https://arxiv.org/abs/2403.15042)
44. [LLM Agent Operating System](https://arxiv.org/abs/2403.16971)
45. [The Unreasonable Ineffectiveness of the Deeper Layers](https://arxiv.org/abs/2403.17887)
46. [BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text](https://arxiv.org/abs/2403.18421)
47. [ViTAR: Vision Transformer with Any Resolution](https://arxiv.org/abs/2403.18361)
48. [Long-form Factuality in Large Language Models](https://arxiv.org/abs/2403.18802)
49. [Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models](https://arxiv.org/abs/2403.18814)
50. [LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning](https://arxiv.org/abs/2403.17919)
51. [Mechanistic Design and Scaling of Hybrid Architectures](https://arxiv.org/abs/2403.17844)
52. [MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions](https://arxiv.org/abs/2403.19651)
53. [Model Stock: All We Need Is Just a Few Fine-Tuned Models](https://arxiv.org/abs/2403.19522)

## April 2024

1. [Do Language Models Plan Ahead for Future Tokens?](https://arxiv.org/abs/2404.00859)
2. [Bigger is not Always Better: Scaling Properties of Latent Diffusion Models](https://arxiv.org/abs/2404.01367)
3. [The Fine Line: Navigating Large Language Model Pretraining with Down-streaming Capability Analysis](https://arxiv.org/abs/2404.01204)
4. [Diffusion-RWKV: Scaling RWKV-Like Architectures for Diffusion Models](https://arxiv.org/abs/2404.04478)
5. [Mixture-of-Depths: Dynamically Allocating Compute in Transformer-Based Language Models](https://arxiv.org/abs/2404.02258)
6. [Long-context LLMs Struggle with Long In-context Learning](https://arxiv.org/abs/2404.02060)
7. [Emergent Abilities in Reduced-Scale Generative Language Models](https://arxiv.org/abs/2404.02204)
8. [Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks](https://arxiv.org/abs/2404.02151)
9. [On the Scalability of Diffusion-based Text-to-Image Generation](https://arxiv.org/abs/2404.02883)
10. [BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models](https://arxiv.org/abs/2404.02827)
11. [Cross-Attention Makes Inference Cumbersome in Text-to-Image Diffusion Models](https://arxiv.org/abs/2404.02747)
12. [Direct Nash Optimization: Teaching Language Models to Self-Improve with General Preferences](https://arxiv.org/abs/2404.02151)
13. [Training LLMs over Neurally Compressed Text](https://arxiv.org/abs/2404.03626)
14. [CantTalkAboutThis: Aligning Language Models to Stay on Topic in Dialogues](https://arxiv.org/abs/2404.03820)
15. [ReFT: Representation Finetuning for Language Models](https://arxiv.org/abs/2404.03592)
16. [Verifiable by Design: Aligning Language Models to Quote from Pre-Training Data](https://arxiv.org/abs/2404.03862)
17. [Sigma: Siamese Mamba Network for Multi-Modal Semantic Segmentation](https://arxiv.org/abs/2404.04256)
18. [AutoCodeRover: Autonomous Program Improvement](https://arxiv.org/abs/2404.05427)
19. [Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence](https://arxiv.org/abs/2404.05892)
20. [CodecLM: Aligning Language Models with Tailored Synthetic Data](https://arxiv.org/abs/2404.05875)
21. [MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies](https://arxiv.org/abs/2404.06395)
22. [Elephants Never Forget: Memorization and Learning of Tabular Data in Large Language Models](https://arxiv.org/abs/2404.06209)
23. [LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders](https://arxiv.org/abs/2404.05961)
24. [Adapting LLaMA Decoder to Vision Transformer](https://arxiv.org/abs/2404.06773)
25. [Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention](https://arxiv.org/abs/2404.07143)
26. [LLoCO: Learning Long Contexts Offline](https://arxiv.org/abs/2404.07979)
27. [JetMoE: Reaching Llama2 Performance with 0.1M Dollars](https://arxiv.org/abs/2404.07413)
28. [Best Practices and Lessons Learned on Synthetic Data for Language Models](https://arxiv.org/abs/2404.07503)
29. [Rho-1: Not All Tokens Are What You Need](https://arxiv.org/abs/2404.07965)
30. [Pre-training Small Base LMs with Fewer Tokens](https://arxiv.org/abs/2404.08634)
31. [Dataset Reset Policy Optimization for RLHF](https://arxiv.org/abs/2404.08495)
32. [LLM In-Context Recall is Prompt Dependent](https://arxiv.org/abs/2404.08865)
33. [State Space Model for New-Generation Network Alternative to Transformers: A Survey](https://arxiv.org/abs/2404.09516)
34. [Chinchilla Scaling: A Replication Attempt](https://arxiv.org/abs/2404.10102)
35. [Learn Your Reference Model for Real Good Alignment](https://arxiv.org/abs/2404.09656)
36. [Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study](https://arxiv.org/abs/2404.10719)
37. [Scaling (Down) CLIP: A Comprehensive Analysis of Data, Architecture, and Training Strategies](https://arxiv.org/abs/2404.08197)
38. [How Faithful Are RAG Models? Quantifying the Tug-of-War Between RAG and LLMs' Internal Prior](https://arxiv.org/abs/2404.10198)
39. [A Survey on Retrieval-Augmented Text Generation for Large Language Models](https://arxiv.org/abs/2404.10981)
40. [When LLMs are Unfit Use FastFit: Fast and Effective Text Classification with Many Classes](https://arxiv.org/abs/2404.12365)
41. [Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing](https://arxiv.org/abs/2404.12253)
42. [OpenBezoar: Small, Cost-Effective and Open Models Trained on Mixes of Instruction Data](https://arxiv.org/abs/2404.12195)
43. [The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions](https://arxiv.org/abs/2404.13208)
44. [How Good Are Low-bit Quantized LLaMA3 Models? An Empirical Study](https://arxiv.org/abs/2404.14047)
45. [Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone](https://arxiv.org/abs/2404.14219)
46. [OpenELM: An Efficient Language Model Family with Open-source Training and Inference Framework](https://arxiv.org/abs/2404.14619)
47. [A Survey on Self-Evolution of Large Language Models](https://arxiv.org/abs/2404.14662)
48. [Multi-Head Mixture-of-Experts](https://arxiv.org/abs/2404.15045)
49. [NExT: Teaching Large Language Models to Reason about Code Execution](https://arxiv.org/abs/2404.14662)
50. [Graph Machine Learning in the Era of Large Language Models (LLMs)](https://arxiv.org/abs/2404.14928)
51. [Retrieval Head Mechanistically Explains Long-Context Factuality](https://arxiv.org/abs/2404.15574)
52. [Layer Skip: Enabling Early Exit Inference and Self-Speculative Decoding](https://arxiv.org/abs/2404.16710)
53. [Make Your LLM Fully Utilize the Context](https://arxiv.org/abs/2404.16811)
54. [LoRA Land: 310 Fine-tuned LLMs that Rival GPT-4, A Technical Report](https://arxiv.org/abs/2405.00732)
55. [Better & Faster Large Language Models via Multi-token Prediction](https://arxiv.org/abs/2404.19737)
56. [RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing](https://arxiv.org/abs/2404.19543)
57. [A Primer on the Inner Workings of Transformer-based Language Models](https://arxiv.org/abs/2405.00208)
58. [When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively](https://arxiv.org/abs/2404.19705)
59. [KAN: Kolmogorovâ€“Arnold Networks](https://arxiv.org/abs/2404.19756)

## May 2024

1. [Is Bigger Edit Batch Size Always Better? An Empirical Study on Model Editing with Llama-3](https://arxiv.org/abs/2405.00664)
2. [Self-Play Preference Optimization for Language Model Alignment](https://arxiv.org/abs/2405.00675)
3. [A Careful Examination of Large Language Model Performance on Grade School Arithmetic](https://arxiv.org/abs/2405.00332)
4. [Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models](https://arxiv.org/abs/2405.01535)
5. [What Matters When Building Vision-Language Models?](https://arxiv.org/abs/2405.02246)
6. [Is Flash Attention Stable?](https://arxiv.org/abs/2405.02803)
7. [vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention](https://arxiv.org/abs/2405.04437)
8. [xLSTM: Extended Long Short-Term Memory](https://arxiv.org/abs/2405.04517)
9. [You Only Cache Once: Decoder-Decoder Architectures for Language Models](https://arxiv.org/abs/2405.05254)
10. [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://arxiv.org/abs/2405.04434)
11. [Fishing for Magikarp: Automatically Detecting Under-trained Tokens in Large Language Models](https://arxiv.org/abs/2405.05417)
12. [Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?](https://arxiv.org/abs/2405.05904)
13. [Value Augmented Sampling for Language Model Alignment and Personalization](https://arxiv.org/abs/2405.06639)
14. [PHUDGE: Phi-3 as Scalable Judge](https://arxiv.org/abs/2405.08029)
15. [RLHF Workflow: From Reward Modeling to Online RLHF](https://arxiv.org/abs/2405.07863)
16. [LoRA Learns Less and Forgets Less](https://arxiv.org/abs/2405.09673)
17. [Xmodel-VLM: A Simple Baseline for Multimodal Vision Language Model](https://arxiv.org/abs/2405.09215)
18. [Chameleon: Mixed-Modal Early-Fusion Foundation Models](https://arxiv.org/abs/2405.09818)
19. [Towards Modular LLMs by Building and Reusing a Library of LoRAs](https://arxiv.org/abs/2405.11157)
20. [SLAB: Efficient Transformers with Simplified Linear Attention and Progressive Re-parameterized Batch Normalization](https://arxiv.org/abs/2405.11582)
21. [MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2405.12130)
22. [Attention as an RNN](https://arxiv.org/abs/2405.13956)
23. [Dense Connector for MLLMs](https://arxiv.org/abs/2405.13800)
24. [AlignGPT: Multi-modal Large Language Models with Adaptive Alignment Capability](https://arxiv.org/abs/2405.14129)
25. [SimPO: Simple Preference Optimization with a Reference-Free Reward](https://arxiv.org/abs/2405.14734)
26. [Instruction Tuning With Loss Over Instructions](https://arxiv.org/abs/2405.14394)
27. [The Road Less Scheduled](https://arxiv.org/abs/2405.15682)
28. [Stacking Your Transformers: A Closer Look at Model Growth for Efficient LLM Pre-Training](https://arxiv.org/abs/2405.15319)
29. [gzip Predicts Data-dependent Scaling Laws](https://arxiv.org/abs/2405.16684)
30. [Trans-LoRA: Towards Data-free Transferable Parameter Efficient Finetuning](https://arxiv.org/abs/2405.17258)
31. [VeLoRA: Memory Efficient Training using Rank-1 Sub-Token Projections](https://arxiv.org/abs/2405.17991)
32. [LLaMA-NAS: Efficient Neural Architecture Search for Large Language Models](https://arxiv.org/abs/2405.18377)
33. [Contextual Position Encoding: Learning to Count What's Important](https://arxiv.org/abs/2405.18719)

## June 2024

1. [Show, Don't Tell: Aligning Language Models with Demonstrated Feedback](https://arxiv.org/abs/2406.00888)
2. [Skywork-MoE: A Deep Dive into Training Techniques for Mixture-of-Experts Language Models](https://arxiv.org/abs/2406.06563)
3. [OLoRA: Orthonormal Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2406.01775)
4. [The Geometry of Categorical and Hierarchical Concepts in Large Language Models](https://arxiv.org/abs/2406.01506)
5. [Towards Scalable Automated Alignment of LLMs: A Survey](https://arxiv.org/abs/2406.01252)
6. [Scalable MatMul-free Language Modeling](https://arxiv.org/abs/2406.02528)
7. [Block Transformer: Global-to-Local Language Modeling for Fast Inference](https://arxiv.org/abs/2406.02657)
8. [Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models](https://arxiv.org/abs/2406.04271)
9. [The Prompt Report: A Systematic Survey of Prompting Techniques](https://arxiv.org/abs/2406.06608)
10. [Transformers Need Glasses! Information Over-Squashing in Language Tasks](https://arxiv.org/abs/2406.04267)
11. [Are We Done with MMLU?](https://arxiv.org/abs/2406.04127)
12. [Step-aware Preference Optimization: Aligning Preference with Denoising Performance at Each Step](https://arxiv.org/abs/2406.04314)
13. [Boosting Large-scale Parallel Training Efficiency with C4: A Communication-Driven Approach](https://arxiv.org/abs/2406.04594)
14. [CRAG -- Comprehensive RAG Benchmark](https://arxiv.org/abs/2406.04744)
15. [WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild](https://arxiv.org/abs/2406.04770)
16. [Mixture-of-Agents Enhances Large Language Model Capabilities](https://arxiv.org/abs/2406.04692)
17. [BERTs are Generative In-Context Learners](https://arxiv.org/abs/2406.04823)
18. [3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and Less Hallucination](https://arxiv.org/abs/2406.05132)
19. [Creativity Has Left the Chat: The Price of Debiasing Language Models](https://arxiv.org/abs/2406.05587)
20. [Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation](https://arxiv.org/abs/2406.06525)
21. [Margin-aware Preference Optimization for Aligning Diffusion Models Without Reference](https://arxiv.org/abs/2406.06424)
22. [Husky: A Unified, Open-Source Language Agent for Multi-Step Reasoning](https://arxiv.org/abs/2406.06469)
23. [Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters](https://arxiv.org/abs/2406.05955)
24. [Self-Tuning: Instructing LLMs to Effectively Acquire New Knowledge through Self-Teaching](https://arxiv.org/abs/2406.06326)
25. [An Image is Worth 32 Tokens for Reconstruction and Generation](https://arxiv.org/abs/2406.07550)
26. [TextGrad: Automatic "Differentiation" via Text](https://arxiv.org/abs/2406.07496)
27. [Simple and Effective Masked Diffusion Language Models](https://arxiv.org/abs/2406.07524)
28. [Never Miss A Beat: An Efficient Recipe for Context Window Extension of Large Language Models with Consistent "Middle" Enhancement](https://arxiv.org/abs/2406.07138)
29. [Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling](https://arxiv.org/abs/2406.07522)
30. [Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing](https://arxiv.org/abs/2406.08464)
31. [What If We Recaption Billions of Web Images with LLaMA-3?](https://arxiv.org/abs/2406.08478)
32. [Large Language Model Unlearning via Embedding-Corrupted Prompts](https://arxiv.org/abs/2406.07933)
33. [Large Language Models Must Be Taught to Know What They Don't Know](https://arxiv.org/abs/2406.08391)
34. [An Empirical Study of Mamba-based Language Models](https://arxiv.org/abs/2406.07887)
35. [Discovering Preference Optimization Algorithms with and for Large Language Models](https://arxiv.org/abs/2406.08414)
36. [Transformers Meet Neural Algorithmic Reasoners](https://arxiv.org/abs/2406.09308)
37. [MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer Decoding](https://arxiv.org/abs/2406.09297)
38. [An Image is Worth More Than 16x16 Patches: Exploring Transformers on Individual Pixels](https://arxiv.org/abs/2406.09415)
39. [FouRA: Fourier Low Rank Adaptation](https://arxiv.org/abs/2406.08798)
40. [Bootstrapping Language Models with DPO Implicit Rewards](https://arxiv.org/abs/2406.09760)
41. [Be like a Goldfish, Don't Memorize! Mitigating Memorization in Generative LLMs](https://arxiv.org/abs/2406.10209)
42. [Regularizing Hidden States Enables Learning Generalizable Reward Model for LLMs](https://arxiv.org/abs/2406.10216)
43. [THEANINE: Revisiting Memory Management in Long-term Conversations with Timeline-augmented Response Generation](https://arxiv.org/abs/2406.10996)
44. [Task Me Anything](https://arxiv.org/abs/2406.11775)
45. [How Do Large Language Models Acquire Factual Knowledge During Pretraining?](https://arxiv.org/abs/2406.11813)
46. [mDPO: Conditional Preference Optimization for Multimodal Large Language Models](https://arxiv.org/abs/2406.11839)
47. [Nemotron-4 340B Technical Report](https://arxiv.org/abs/2406.11704)
48. [DataComp-LM: In Search of the Next Generation of Training Sets for Language Models](https://arxiv.org/abs/2406.11794)
49. [Tokenization Falling Short: The Curse of Tokenization](https://arxiv.org/abs/2406.11687)
50. [DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence](https://arxiv.org/abs/2406.11931)
51. [Unveiling Encoder-Free Vision-Language Models](https://arxiv.org/abs/2406.11832)
52. [Iterative Length-Regularized Direct Preference Optimization: A Case Study on Improving 7B Language Models to GPT-4 Level](https://arxiv.org/abs/2406.11817)
53. [HARE: HumAn pRiors, a key to small language model Efficiency](https://arxiv.org/abs/2406.11410)
54. [Measuring memorization in RLHF for code completion](https://arxiv.org/abs/2406.11715)
55. [Self-MoE: Towards Compositional Large Language Models with Self-Specialized Experts](https://arxiv.org/abs/2406.12034)
56. [From RAGs to Rich Parameters: Probing How Language Models Utilize External Knowledge Over Parametric Information for Factual Queries](https://arxiv.org/abs/2406.12824)
57. [Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges](https://arxiv.org/abs/2406.12624)
58. [Can Long-Context Language Models Subsume Retrieval, RAG, SQL, and More?](https://arxiv.org/abs/2406.13121)
59. [Instruction Pre-Training: Language Models are Supervised Multitask Learners](https://arxiv.org/abs/2406.14491)
60. [Can LLMs Learn by Teaching? A Preliminary Study](https://arxiv.org/abs/2406.14629)
61. [A Tale of Trust and Accuracy: Base vs. Instruct LLMs in RAG Systems](https://arxiv.org/abs/2406.14972)
62. [LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs](https://arxiv.org/abs/2406.15319)
63. [MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression](https://arxiv.org/abs/2406.14909)
64. [Efficient Continual Pre-training by Mitigating the Stability Gap](https://arxiv.org/abs/2406.14833)
65. [Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers](https://arxiv.org/abs/2406.16747)
66. [WARP: On the Benefits of Weight Averaged Rewarded Policies](https://arxiv.org/abs/2406.16768)
67. [Adam-mini: Use Fewer Learning Rates To Gain More](https://arxiv.org/abs/2406.16793)
68. [The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale](https://arxiv.org/abs/2406.17557)
69. [LongIns: A Challenging Long-context Instruction-based Exam for LLMs](https://arxiv.org/abs/2406.17588)
70. [Following Length Constraints in Instructions](https://arxiv.org/abs/2406.17744)
71. [A Closer Look into Mixture-of-Experts in Large Language Models](https://arxiv.org/abs/2406.18219)
72. [RouteLLM: Learning to Route LLMs with Preference Data](https://arxiv.org/abs/2406.18665)
73. [Step-DPO: Step-wise Preference Optimization for Long-chain Reasoning of LLMs](https://arxiv.org/abs/2406.18629)
74. [Dataset Size Recovery from LoRA Weights](https://arxiv.org/abs/2406.19395)
75. [From Artificial Needles to Real Haystacks: Improving Retrieval Capabilities in LLMs by Finetuning on Synthetic Data](https://arxiv.org/abs/2406.19292)
76. [Changing Answer Order Can Decrease MMLU Accuracy](https://arxiv.org/abs/2406.19470)
77. [Direct Preference Knowledge Distillation for Large Language Models](https://arxiv.org/abs/2406.19774)
78. [LLM Critics Help Catch LLM Bugs](https://arxiv.org/abs/2407.00215)
79. [Scaling Synthetic Data Creation with 1,000,000,000 Personas](https://arxiv.org/abs/2406.20094)

## July 2024

1. [LLM See, LLM Do: Guiding Data Generation to Target Non-Differentiable Objectives](https://arxiv.org/abs/2407.01490)
2. [Searching for Best Practices in Retrieval-Augmented Generation](https://arxiv.org/abs/2407.01219)
3. [Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models](https://arxiv.org/abs/2407.01906)
4. [Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion](https://arxiv.org/abs/2407.01392)
5. [Eliminating Position Bias of Language Models: A Mechanistic Approach](https://arxiv.org/abs/2407.01100)
6. [JMInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention](https://arxiv.org/abs/2407.02490)
7. [TokenPacker: Efficient Visual Projector for Multimodal LLM](https://arxiv.org/abs/2407.02392)
8. [Reasoning in Large Language Models: A Geometric Perspective](https://arxiv.org/abs/2407.02678)
9. [RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs](https://arxiv.org/abs/2407.02485)
10. [AgentInstruct: Toward Generative Teaching with Agentic Flows](https://arxiv.org/abs/2407.03502)
11. [HEMM: Holistic Evaluation of Multimodal Foundation Models](https://arxiv.org/abs/2407.03418)
12. [Mixture of A Million Experts](https://arxiv.org/abs/2407.04153)
13. [Learning to (Learn at Test Time): RNNs with Expressive Hidden States](https://arxiv.org/abs/2407.04620)
14. [Vision Language Models Are Blind](https://arxiv.org/abs/2407.06581)
15. [Self-Recognition in Language Models](https://arxiv.org/abs/2407.06946)
16. [Inference Performance Optimization for Large Language Models on CPUs](https://arxiv.org/abs/2407.07304)
17. [FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision](https://arxiv.org/abs/2407.08608)
18. [SpreadsheetLLM: Encoding Spreadsheets for Large Language Models](https://arxiv.org/abs/2407.09025)
19. [New Desiderata for Direct Preference Optimization](https://arxiv.org/abs/2407.09072)
20. [Context Embeddings for Efficient Answer Generation in RAG](https://arxiv.org/abs/2407.09252)
21. [Qwen2 Technical Report](https://arxiv.org/abs/2407.10671)
22. [The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore Non-Determinism](https://arxiv.org/abs/2407.10457)
23. [From GaLore to WeLore: How Low-Rank Weights Non-uniformly Emerge from Low-Rank Gradients](https://arxiv.org/abs/2407.11239)
24. [GoldFinch: High Performance RWKV/Transformer Hybrid with Linear Pre-Fill and Extreme KV-Cache Compression](https://arxiv.org/abs/2407.12077)
25. [Scaling Diffusion Transformers to 16 Billion Parameters](https://arxiv.org/abs/2407.11633)
26. [NeedleBench: Can LLMs Do Retrieval and Reasoning in 1 Million Context Window?](https://arxiv.org/abs/2407.11963)
27. [Patch-Level Training for Large Language Models](https://arxiv.org/abs/2407.12665)
28. [LMMs-Eval: Reality Check on the Evaluation of Large Multimodal Models](https://arxiv.org/abs/2407.12772)
29. [A Survey of Prompt Engineering Methods in Large Language Models for Different NLP Tasks](https://arxiv.org/abs/2407.12994)
30. [Spectra: A Comprehensive Study of Ternary, Quantized, and FP16 Language Models](https://arxiv.org/abs/2407.12327)
31. [Attention Overflow: Language Model Input Blur during Long-Context Missing Items Recommendation](https://arxiv.org/abs/2407.13481)
32. [Weak-to-Strong Reasoning](https://arxiv.org/abs/2407.13647)
33. [Understanding Reference Policies in Direct Preference Optimization](https://arxiv.org/abs/2407.13709)
34. [Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies](https://arxiv.org/abs/2407.13623)
35. [BOND: Aligning LLMs with Best-of-N Distillation](https://arxiv.org/abs/2407.14622)
36. [Compact Language Models via Pruning and Knowledge Distillation](https://arxiv.org/abs/2407.14679)
37. [LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference](https://arxiv.org/abs/2407.14057)
38. [Mini-Sequence Transformer: Optimizing Intermediate Memory for Long Sequences Training](https://arxiv.org/abs/2407.15892)
39. [DDK: Distilling Domain Knowledge for Efficient Large Language Models](https://arxiv.org/abs/2407.16154)
40. [Generation Constraint Scaling Can Mitigate Hallucination](https://arxiv.org/abs/2407.16908)
41. [Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach](https://arxiv.org/abs/2407.16833)
42. [Course-Correction: Safety Alignment Using Synthetic Preferences](https://arxiv.org/abs/2407.16637)
43. [Data Mixture Inference: What do BPE Tokenizers Reveal about their Training Data?](https://arxiv.org/abs/2407.16607)
44. [Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge](https://arxiv.org/abs/2407.19594)
45. [Improving Retrieval Augmented Language Model with Self-Reasoning](https://arxiv.org/abs/2407.19813)
46. [Apple Intelligence Foundation Language Models](https://arxiv.org/abs/2407.21075)
47. [ThinK: Thinner Key Cache by Query-Driven Pruning](https://arxiv.org/abs/2407.21018)
48. [The Llama 3 Herd of Models](https://arxiv.org/abs/2407.21783)
49. [Gemma 2: Improving Open Language Models at a Practical Size](https://arxiv.org/abs/2408.00118)

## August 2024

1. [SAM 2: Segment Anything in Images and Videos](https://arxiv.org/abs/2408.00714)
2. [POA: Pre-training Once for Models of All Sizes](https://arxiv.org/abs/2408.01031)
3. [RAGEval: Scenario Specific RAG Evaluation Dataset Generation Framework](https://arxiv.org/abs/2408.01262)
4. [A Survey of Mamba](https://arxiv.org/abs/2408.01129)
5. [MiniCPM-V: A GPT-4V Level MLLM on Your Phone](https://arxiv.org/abs/2408.01800)
6. [RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation](https://arxiv.org/abs/2408.02545)
7. [Self-Taught Evaluators](https://arxiv.org/abs/2408.02666)
8. [BioMamba: A Pre-trained Biomedical Language Representation Model Leveraging Mamba](https://arxiv.org/abs/2408.02600)
9. [EXAONE 3.0 7.8B Instruction Tuned Language Model](https://arxiv.org/abs/2408.03541)
10. [1.5-Pints Technical Report: Pretraining in Days, Not Months -- Your Language Model Thrives on Quality Data](https://arxiv.org/abs/2408.03506)
11. [Conversational Prompt Engineering](https://arxiv.org/abs/2408.04560)
12. [Trans-Tokenization and Cross-lingual Vocabulary Transfers: Language Adaptation of LLMs for Low-Resource NLP](https://arxiv.org/abs/2408.04303)
13. [The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery](https://arxiv.org/abs/2408.06292)
14. [Hermes 3 Technical Report](https://arxiv.org/abs/2408.12570)
15. [Customizing Language Models with Instance-wise LoRA for Sequential Recommendation](https://arxiv.org/abs/2408.10159)
16. [Enhancing Robustness in Large Language Models: Prompting for Mitigating the Impact of Irrelevant Information](https://arxiv.org/abs/2408.10615)
17. [To Code, or Not To Code? Exploring Impact of Code in Pre-training](https://arxiv.org/abs/2408.10914)
18. [LLM Pruning and Distillation in Practice: The Minitron Approach](https://arxiv.org/abs/2408.11796)
19. [Jamba-1.5: Hybrid Transformer-Mamba Models at Scale](https://arxiv.org/abs/2408.12570)
20. [Controllable Text Generation for Large Language Models: A Survey](https://arxiv.org/abs/2408.12599)
21. [Multi-Layer Transformers Gradient Can be Approximated in Almost Linear Time](https://arxiv.org/abs/2408.13233)
22. [A Practitioner's Guide to Continual Multimodal Pretraining](https://arxiv.org/abs/2408.14471)
23. [Building and Better Understanding Vision-Language Models: Insights and Future Directions](https://arxiv.org/abs/2408.12637)
24. [CURLoRA: Stable LLM Continual Fine-Tuning and Catastrophic Forgetting Mitigation](https://arxiv.org/abs/2408.14572)
25. [The Mamba in the Llama: Distilling and Accelerating Hybrid Models](https://arxiv.org/abs/2408.15237)
26. [ReMamba: Equip Mamba with Effective Long-Sequence Modeling](https://arxiv.org/abs/2408.15496)
27. [Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling](https://arxiv.org/abs/2408.16737)
28. [LongRecipe: Recipe for Efficient Long Context Generalization in Large Language Models](https://arxiv.org/abs/2409.00509)

## September 2024

1. [OLMoE: Open Mixture-of-Experts Language Models](https://arxiv.org/abs/2409.02060)
2. [In Defense of RAG in the Era of Long-Context Language Models](https://arxiv.org/abs/2409.01666)
3. [Attention Heads of Large Language Models: A Survey](https://arxiv.org/abs/2409.03752)
4. [LongCite: Enabling LLMs to Generate Fine-grained Citations in Long-context QA](https://arxiv.org/abs/2409.02897)
5. [How Do Your Code LLMs Perform? Empowering Code Instruction Tuning with High-Quality Data](https://arxiv.org/abs/2409.03810)
6. [Theory, Analysis, and Best Practices for Sigmoid Self-Attention](https://arxiv.org/abs/2409.04431)
7. [LLaMA-Omni: Seamless Speech Interaction with Large Language Models](https://arxiv.org/abs/2409.06666)
8. [What is the Role of Small Models in the LLM Era: A Survey](https://arxiv.org/abs/2409.06857)
9. [Policy Filtration in RLHF to Fine-Tune LLM for Code Generation](https://arxiv.org/abs/2409.06957)
10. [RetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval](https://arxiv.org/abs/2409.10516)
11. [Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement](https://arxiv.org/abs/2409.12122)
12. [Qwen2.5-Coder Technical Report](https://arxiv.org/abs/2409.12186)
13. [Instruction Following without Instruction Tuning](https://arxiv.org/abs/2409.14254)
14. [Is Preference Alignment Always the Best Option to Enhance LLM-Based Translation? An Empirical Analysis](https://arxiv.org/abs/2409.20059)
15. [The Perfect Blend: Redefining RLHF with Mixture of Judges](https://arxiv.org/abs/2409.20370)

## October 2024

1. [Addition is All You Need for Energy-efficient Language Models](https://arxiv.org/abs/2410.00907)
2. [Quantifying Generalization Complexity for Large Language Models](https://arxiv.org/abs/2410.01769)
3. [When a language model is optimized for reasoning, does it still show embers of autoregression? An analysis of OpenAI o1](https://arxiv.org/abs/2410.01792)
4. [Were RNNs All We Needed?](https://arxiv.org/abs/2410.01201)
5. [Selective Attention Improves Transformer](https://arxiv.org/abs/2410.02703)
6. [LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations](https://arxiv.org/abs/2410.02707)
7. [LLaVA-Critic: Learning to Evaluate Multimodal Models](https://arxiv.org/abs/2410.02712)
8. [Differential Transformer](https://arxiv.org/abs/2410.05258)
9. [GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models](https://arxiv.org/abs/2410.05229)
10. [ARIA: An Open Multimodal Native Mixture-of-Experts Model](https://arxiv.org/abs/2410.05993)
11. [O1 Replication Journey: A Strategic Progress Report -- Part 1](https://arxiv.org/abs/2410.18982)
12. [Long-Context LLMs Meet RAG: Overcoming Challenges for Long Inputs in RAG](https://arxiv.org/abs/2410.05983)
13. [From Generalist to Specialist: Adapting Vision Language Models via Task-Specific Visual Instruction Tuning](https://arxiv.org/abs/2410.06456)
14. [KV Prediction for Improved Time to First Token](https://arxiv.org/abs/2410.08391)
15. [Baichuan-Omni Technical Report](https://arxiv.org/abs/2410.08565)
16. [MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models](https://arxiv.org/abs/2410.10139)
17. [LOKI: A Comprehensive Synthetic Data Detection Benchmark using Large Multimodal Models](https://arxiv.org/abs/2410.09732)
18. [AFlow: Automating Agentic Workflow Generation](https://arxiv.org/abs/2410.10762)
19. [Toward General Instruction-Following Alignment for Retrieval-Augmented Generation](https://arxiv.org/abs/2410.09584)
20. [Pre-training Distillation for Large Language Models: A Design Space Exploration](https://arxiv.org/abs/2410.16215)
21. [MIA-DPO: Multi-Image Augmented Direct Preference Optimization For Large Vision-Language Models](https://arxiv.org/abs/2410.17637)
22. [Scalable Ranked Preference Optimization for Text-to-Image Generation](https://arxiv.org/abs/2410.18013)
23. [Scaling Diffusion Language Models via Adaptation from Autoregressive Models](https://arxiv.org/abs/2410.17891)
24. [Hybrid Preferences: Learning to Route Instances for Human vs. AI Feedback](https://arxiv.org/abs/2410.19133)
25. [Counting Ability of Large Language Models and Impact of Tokenization](https://arxiv.org/abs/2410.19730)
26. [A Survey of Small Language Models](https://arxiv.org/abs/2410.20011)
27. [Accelerating Direct Preference Optimization with Prefix Sharing](https://arxiv.org/abs/2410.20305)
28. [Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on Tasks where Thinking Makes Humans Worse](https://arxiv.org/abs/2410.21333)
29. [LongReward: Improving Long-context Large Language Models with AI Feedback](https://arxiv.org/abs/2410.21252)
30. [ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference](https://arxiv.org/abs/2410.21465)
31. [Beyond Text: Optimizing RAG with Multimodal Inputs for Industrial Applications](https://arxiv.org/abs/2410.21943)
32. [CORAL: Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation](https://arxiv.org/abs/2410.23090)
33. [What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking: A Gradient Perspective](https://arxiv.org/abs/2410.23743)
34. [GPT or BERT: why not both?](https://arxiv.org/abs/2410.24159)
35. [Language Models can Self-Lengthen to Generate Long Texts](https://arxiv.org/abs/2410.23933)

## November 2024

1. [Adding Error Bars to Evals: A Statistical Approach to Language Model Evaluations](https://arxiv.org/abs/2411.00640)
2. [Adapting While Learning: Grounding LLMs for Scientific Problems with Intelligent Tool Usage Adaptation](https://arxiv.org/abs/2411.00412)
3. [Multi-expert Prompting Improves Reliability, Safety, and Usefulness of Large Language Models](https://arxiv.org/abs/2411.00492)
4. [Sample-Efficient Alignment for LLMs](https://arxiv.org/abs/2411.01493)
5. [A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness](https://arxiv.org/abs/2411.03350)
6. ["Give Me BF16 or Give Me Death"? Accuracy-Performance Trade-Offs in LLM Quantization](https://arxiv.org/abs/2411.02355)
7. [Parameter-Efficient Fine-Tuning of Large Language Models for Unit Test Generation: An Empirical Study](https://arxiv.org/abs/2411.02462)
8. [HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge in RAG Systems](https://arxiv.org/abs/2411.02959)
9. [Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM Data Contamination](https://arxiv.org/abs/2411.03823)
10. [Language Models are Hidden Reasoners: Unlocking Latent Reasoning Capabilities via Self-Rewarding](https://arxiv.org/abs/2411.04282)
11. [Number Cookbook: Number Understanding of Language Models and How to Improve It](https://arxiv.org/abs/2411.03766)
12. [Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models](https://arxiv.org/abs/2411.04996)
13. [BitNet a4.8: 4-bit Activations for 1-bit LLMs](https://arxiv.org/abs/2411.04965)
14. [Scaling Laws for Precision](https://arxiv.org/abs/2411.04330)
15. [Energy Efficient Protein Language Models: Leveraging Small Language Models with LoRA for Controllable Protein Generation](https://arxiv.org/abs/2411.05966)
16. [Balancing Pipeline Parallelism with Vocabulary Parallelism](https://arxiv.org/abs/2411.05288)
17. [Toward Optimal Search and Retrieval for RAG](https://arxiv.org/abs/2411.07396)
18. [Large Language Models Can Self-Improve in Long-context Reasoning](https://arxiv.org/abs/2411.08147)
19. [Stronger Models are NOT Stronger Teachers for Instruction Tuning](https://arxiv.org/abs/2411.07133)
20. [Direct Preference Optimization Using Sparse Feature-Level Constraints](https://arxiv.org/abs/2411.07618)
21. [Cut Your Losses in Large-Vocabulary Language Models](https://arxiv.org/abs/2411.09009)
22. [Does Prompt Formatting Have Any Impact on LLM Performance?](https://arxiv.org/abs/2411.10541)
23. [SymDPO: Boosting In-Context Learning of Large Multimodal Models with Symbol Demonstration Direct Preference Optimization](https://arxiv.org/abs/2411.11909)
24. [SageAttention2 Technical Report: Accurate 4 Bit Attention for Plug-and-play Inference Acceleration](https://arxiv.org/abs/2411.10958)
25. [Bi-Mamba: Towards Accurate 1-Bit State Space Models](https://arxiv.org/abs/2411.11843)
26. [RedPajama: an Open Dataset for Training Large Language Models](https://arxiv.org/abs/2411.12372)
27. [Hymba: A Hybrid-head Architecture for Small Language Models](https://arxiv.org/abs/2411.13676)
28. [Loss-to-Loss Prediction: Scaling Laws for All Datasets](https://arxiv.org/abs/2411.12925)
29. [When Precision Meets Position: BFloat16 Breaks Down RoPE in Long-Context Training](https://arxiv.org/abs/2411.13476)
30. [Multimodal Autoregressive Pre-training of Large Vision Encoders](https://arxiv.org/abs/2411.14402)
31. [Natural Language Reinforcement Learning](https://arxiv.org/abs/2411.14251)
32. [Large Multi-modal Models Can Interpret Features in Large Multi-modal Models](https://arxiv.org/abs/2411.14982)
33. [TÃœLU 3: Pushing Frontiers in Open Language Model Post-Training](https://arxiv.org/abs/2411.15124)
34. [MME-Survey: A Comprehensive Survey on Evaluation of Multimodal LLMs](https://arxiv.org/abs/2411.15296)
35. [LLMs Do Not Think Step-by-step In Implicit Reasoning](https://arxiv.org/abs/2411.15862)
36. [O1 Replication Journey -- Part 2: Surpassing O1-preview through Simple Distillation, Big Progress or Bitter Lesson?](https://arxiv.org/abs/2411.16489)
37. [Star Attention: Efficient LLM Inference over Long Sequences](https://arxiv.org/abs/2411.17116)
38. [Low-Bit Quantization Favors Undertrained LLMs: Scaling Laws for Quantized LLMs with 100T Training Tokens](https://arxiv.org/abs/2411.17691)
39. [Rethinking Token Reduction in MLLMs: Towards a Unified Paradigm for Training-Free Acceleration](https://arxiv.org/abs/2411.17686)
40. [Reverse Thinking Makes LLMs Stronger Reasoners](https://arxiv.org/abs/2411.19865)
41. [Critical Tokens Matter: Token-Level Contrastive Estimation Enhances LLM's Reasoning Capability](https://arxiv.org/abs/2411.19943)

## December 2024

1. [Switti: Designing Scale-Wise Transformers for Text-to-Image Synthesis](https://arxiv.org/abs/2412.01819)
2. [X-Prompt: Towards Universal In-Context Image Generation in Auto-Regressive Vision Language Foundation Models](https://arxiv.org/abs/2412.01824)
3. [Free Process Rewards without Process Labels](https://arxiv.org/abs/2412.01981)
4. [Scaling Image Tokenizers with Grouped Spherical Quantization](https://arxiv.org/abs/2412.02632)
5. [RARE: Retrieval-Augmented Reasoning Enhancement for Large Language Models](https://arxiv.org/abs/2412.02830)
6. [Perception Tokens Enhance Visual Reasoning in Multimodal Language Models](https://arxiv.org/abs/2412.03548)
7. [Evaluating Language Models as Synthetic Data Generators](https://arxiv.org/abs/2412.03679)
8. [Best-of-N Jailbreaking](https://arxiv.org/abs/2412.03556)
9. [PaliGemma 2: A Family of Versatile VLMs for Transfer](https://arxiv.org/abs/2412.03555)
10. [VisionZip: Longer is Better but Not Necessary in Vision Language Models](https://arxiv.org/abs/2412.04467)
11. [Evaluating and Aligning CodeLLMs on Human Preference](https://arxiv.org/abs/2412.05210)
12. [MAmmoTH-VL: Eliciting Multimodal Reasoning with Instruction Tuning at Scale](https://arxiv.org/abs/2412.05237)
13. [Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling](https://arxiv.org/abs/2412.05271)
14. [LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods](https://arxiv.org/abs/2412.05579)
15. [Does RLHF Scale? Exploring the Impacts From Data, Model, and Method](https://arxiv.org/abs/2412.06000)
16. [Unraveling the Complexity of Memory in RL Agents: An Approach for Classification and Evaluation](https://arxiv.org/abs/2412.06531)
17. [Training Large Language Models to Reason in a Continuous Latent Space](https://arxiv.org/abs/2412.06769)
18. [AutoReason: Automatic Few-Shot Reasoning Decomposition](https://arxiv.org/abs/2412.06975)
19. [Large Concept Models: Language Modeling in a Sentence Representation Space](https://arxiv.org/abs/2412.08821)
20. [Phi-4 Technical Report](https://arxiv.org/abs/2412.08905)
21. [Byte Latent Transformer: Patches Scale Better Than Tokens](https://arxiv.org/abs/2412.09871)
22. [SCBench: A KV Cache-Centric Analysis of Long-Context Methods](https://arxiv.org/abs/2412.10319)
23. [Cultural Evolution of Cooperation among LLM Agents](https://arxiv.org/abs/2412.10270)
24. [DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding](https://arxiv.org/abs/2412.10302)
25. [No More Adam: Learning Rate Scaling at Initialization is All You Need](https://arxiv.org/abs/2412.11768)
26. [Precise Length Control in Large Language Models](https://arxiv.org/abs/2412.11937)
27. [The Open Source Advantage in Large Language Models (LLMs)](https://arxiv.org/abs/2412.12004)
28. [A Survey of Mathematical Reasoning in the Era of Multimodal Large Language Model: Benchmark, Method & Challenges](https://arxiv.org/abs/2412.11936)
29. [Are Your LLMs Capable of Stable Reasoning?](https://arxiv.org/abs/2412.13147)
30. [LLM Post-Training Recipes: Improving Reasoning in LLMs](https://arxiv.org/abs/2412.14135)
31. [Hansel: Output Length Controlling Framework for Large Language Models](https://arxiv.org/abs/2412.14033)
32. [Mind Your Theory: Theory of Mind Goes Deeper Than Reasoning](https://arxiv.org/abs/2412.1363)
33. [Alignment Faking in Large Language Models](https://arxiv.org/abs/2412.14093)
34. [SCOPE: Optimizing Key-Value Cache Compression in Long-Context Generation](https://arxiv.org/abs/2412.13649)
35. [LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-Context Multitasks](https://arxiv.org/abs/2412.15204)
36. [Offline Reinforcement Learning for LLM Multi-Step Reasoning](https://arxiv.org/abs/2412.16145)
37. [Mulberry: Empowering MLLM with O1-like Reasoning and Reflection via Collective Monte Carlo Tree Search](https://arxiv.org/abs/2412.18319)
