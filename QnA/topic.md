# Machine Learning (Q&A)

## Q&A Topics:
1. Supervised vs Unsupervised
2. Bias vs Variance
3. Underfitting vs overfitting
4. Tackle overfitting
5. L1 regularization
6. L2 regularization
7. L1 (Lasso) vs L2 (Ridge)
8. Dropout
9. Why feature reduction / dimensionality reduction
10. How feature reduction / dimensionality reduction
11. AUC ROC
12. No Free Lunch Theorum
13. Empirical Risk
14. Class imbalance tackle
15. Selection bias
16. What is Random Forest? Why "random"?
17. Decision trees vs Logistic regression
18. SVM vs Logistic Regression
19. Kernel trick
20. Primal vs Dual version of classifier
21. Parametric vs Non-parametric
22. KNN 
23. KMeans
24. KNN vs Kmeans
25. Smoothing Time series
26. Gradient descent
27. Backpropagation
28. Regularization
29. Normalization
30. Batch Normalization
31. Vanishing Gradients
32. Exploding gradients

## Other Topics:
1. Dimensionality reduction
2. PCA
3. Kernel PCA
4. Ridge regression
5. L1 vs L2 loss
6. Activation Functions
7. Advantages of RelU
8. Thresholds for a classifier
9. Interpretation of an ROC area under the curve as an integral
10. Confusion matrix

###  More elaborate and extensive Questions:
1. [General Machine Learning](https://github.com/Sroy20/machine-learning-interview-questions/blob/master/list_of_questions_machine_learning.md)
2. [Deep Learning](https://github.com/Sroy20/machine-learning-interview-questions/blob/master/list_of_questions_deep_learning.md)
3. [Mathematics for Machine Learning](https://github.com/Sroy20/machine-learning-interview-questions/blob/master/list_of_questions_mathematics.md)